//
// Copyright Amazon.com Inc. or its affiliates.
// All Rights Reserved.
//
// SPDX-License-Identifier: Apache-2.0
//

// Code generated by smithy-swift-codegen. DO NOT EDIT!

@_spi(SmithyReadWrite) import ClientRuntime
import Foundation
import class SmithyEventStreams.DefaultMessageDecoder
import class SmithyHTTPAPI.HTTPResponse
@_spi(SmithyReadWrite) import class SmithyJSON.Reader
@_spi(SmithyReadWrite) import class SmithyJSON.Writer
import enum ClientRuntime.ErrorFault
import enum Smithy.ClientError
import enum SmithyEventStreamsAPI.MessageType
import enum SmithyReadWrite.ReaderError
@_spi(SmithyReadWrite) import enum SmithyReadWrite.ReadingClosures
@_spi(SmithyReadWrite) import func SmithyReadWrite.listWritingClosure
import protocol AWSClientRuntime.AWSServiceError
import protocol ClientRuntime.HTTPError
import protocol ClientRuntime.ModeledError
@_spi(SmithyReadWrite) import protocol SmithyReadWrite.SmithyReader
@_spi(SmithyReadWrite) import protocol SmithyReadWrite.SmithyWriter
@_spi(SmithyReadWrite) import struct AWSClientRuntime.RestJSONError
@_spi(UnknownAWSHTTPServiceError) import struct AWSClientRuntime.UnknownAWSHTTPServiceError
import struct SmithyEventStreams.DefaultMessageDecoderStream
import struct SmithyEventStreamsAPI.Header
import struct SmithyEventStreamsAPI.Message
import struct SmithyHTTPAPI.Header
import struct SmithyHTTPAPI.Headers
import typealias SmithyEventStreamsAPI.MarshalClosure
import typealias SmithyEventStreamsAPI.UnmarshalClosure

extension TranscribeStreamingClientTypes {
    /// Contains entities identified as personally identifiable information (PII) in your transcription output, along with various associated attributes. Examples include category, confidence score, type, stability score, and start and end times.
    public struct Entity {
        /// The category of information identified. The only category is PII.
        public var category: Swift.String?
        /// The confidence score associated with the identified PII entity in your audio. Confidence scores are values between 0 and 1. A larger value indicates a higher probability that the identified entity correctly matches the entity spoken in your media.
        public var confidence: Swift.Double?
        /// The word or words identified as PII.
        public var content: Swift.String?
        /// The end time, in milliseconds, of the utterance that was identified as PII.
        public var endTime: Swift.Double
        /// The start time, in milliseconds, of the utterance that was identified as PII.
        public var startTime: Swift.Double
        /// The type of PII identified. For example, NAME or CREDIT_DEBIT_NUMBER.
        public var type: Swift.String?

        public init(
            category: Swift.String? = nil,
            confidence: Swift.Double? = nil,
            content: Swift.String? = nil,
            endTime: Swift.Double = 0.0,
            startTime: Swift.Double = 0.0,
            type: Swift.String? = nil
        )
        {
            self.category = category
            self.confidence = confidence
            self.content = content
            self.endTime = endTime
            self.startTime = startTime
            self.type = type
        }
    }

}

extension TranscribeStreamingClientTypes {

    public enum ItemType: Swift.Equatable, Swift.RawRepresentable, Swift.CaseIterable, Swift.Hashable {
        case pronunciation
        case punctuation
        case sdkUnknown(Swift.String)

        public static var allCases: [ItemType] {
            return [
                .pronunciation,
                .punctuation
            ]
        }

        public init?(rawValue: Swift.String) {
            let value = Self.allCases.first(where: { $0.rawValue == rawValue })
            self = value ?? Self.sdkUnknown(rawValue)
        }

        public var rawValue: Swift.String {
            switch self {
            case .pronunciation: return "pronunciation"
            case .punctuation: return "punctuation"
            case let .sdkUnknown(s): return s
            }
        }
    }
}

extension TranscribeStreamingClientTypes {
    /// A word, phrase, or punctuation mark in your transcription output, along with various associated attributes, such as confidence score, type, and start and end times.
    public struct Item {
        /// The confidence score associated with a word or phrase in your transcript. Confidence scores are values between 0 and 1. A larger value indicates a higher probability that the identified item correctly matches the item spoken in your media.
        public var confidence: Swift.Double?
        /// The word or punctuation that was transcribed.
        public var content: Swift.String?
        /// The end time, in milliseconds, of the transcribed item.
        public var endTime: Swift.Double
        /// If speaker partitioning is enabled, Speaker labels the speaker of the specified item.
        public var speaker: Swift.String?
        /// If partial result stabilization is enabled, Stable indicates whether the specified item is stable (true) or if it may change when the segment is complete (false).
        public var stable: Swift.Bool?
        /// The start time, in milliseconds, of the transcribed item.
        public var startTime: Swift.Double
        /// The type of item identified. Options are: PRONUNCIATION (spoken words) and PUNCTUATION.
        public var type: TranscribeStreamingClientTypes.ItemType?
        /// Indicates whether the specified item matches a word in the vocabulary filter included in your request. If true, there is a vocabulary filter match.
        public var vocabularyFilterMatch: Swift.Bool

        public init(
            confidence: Swift.Double? = nil,
            content: Swift.String? = nil,
            endTime: Swift.Double = 0.0,
            speaker: Swift.String? = nil,
            stable: Swift.Bool? = nil,
            startTime: Swift.Double = 0.0,
            type: TranscribeStreamingClientTypes.ItemType? = nil,
            vocabularyFilterMatch: Swift.Bool = false
        )
        {
            self.confidence = confidence
            self.content = content
            self.endTime = endTime
            self.speaker = speaker
            self.stable = stable
            self.startTime = startTime
            self.type = type
            self.vocabularyFilterMatch = vocabularyFilterMatch
        }
    }

}

extension TranscribeStreamingClientTypes {
    /// A list of possible alternative transcriptions for the input audio. Each alternative may contain one or more of Items, Entities, or Transcript.
    public struct Alternative {
        /// Contains entities identified as personally identifiable information (PII) in your transcription output.
        public var entities: [TranscribeStreamingClientTypes.Entity]?
        /// Contains words, phrases, or punctuation marks in your transcription output.
        public var items: [TranscribeStreamingClientTypes.Item]?
        /// Contains transcribed text.
        public var transcript: Swift.String?

        public init(
            entities: [TranscribeStreamingClientTypes.Entity]? = nil,
            items: [TranscribeStreamingClientTypes.Item]? = nil,
            transcript: Swift.String? = nil
        )
        {
            self.entities = entities
            self.items = items
            self.transcript = transcript
        }
    }

}

extension TranscribeStreamingClientTypes {
    /// A wrapper for your audio chunks. Your audio stream consists of one or more audio events, which consist of one or more audio chunks. For more information, see [Event stream encoding](https://docs.aws.amazon.com/transcribe/latest/dg/event-stream.html).
    public struct AudioEvent {
        /// An audio blob that contains the next part of the audio that you want to transcribe. The maximum audio chunk size is 32 KB.
        public var audioChunk: Foundation.Data?

        public init(
            audioChunk: Foundation.Data? = nil
        )
        {
            self.audioChunk = audioChunk
        }
    }

}

extension TranscribeStreamingClientTypes {

    public enum ParticipantRole: Swift.Equatable, Swift.RawRepresentable, Swift.CaseIterable, Swift.Hashable {
        case agent
        case customer
        case sdkUnknown(Swift.String)

        public static var allCases: [ParticipantRole] {
            return [
                .agent,
                .customer
            ]
        }

        public init?(rawValue: Swift.String) {
            let value = Self.allCases.first(where: { $0.rawValue == rawValue })
            self = value ?? Self.sdkUnknown(rawValue)
        }

        public var rawValue: Swift.String {
            switch self {
            case .agent: return "AGENT"
            case .customer: return "CUSTOMER"
            case let .sdkUnknown(s): return s
            }
        }
    }
}

extension TranscribeStreamingClientTypes {
    /// Makes it possible to specify which speaker is on which audio channel. For example, if your agent is the first participant to speak, you would set ChannelId to 0 (to indicate the first channel) and ParticipantRole to AGENT (to indicate that it's the agent speaking).
    public struct ChannelDefinition {
        /// Specify the audio channel you want to define.
        /// This member is required.
        public var channelId: Swift.Int
        /// Specify the speaker you want to define. Omitting this parameter is equivalent to specifying both participants.
        /// This member is required.
        public var participantRole: TranscribeStreamingClientTypes.ParticipantRole?

        public init(
            channelId: Swift.Int = 0,
            participantRole: TranscribeStreamingClientTypes.ParticipantRole? = nil
        )
        {
            self.channelId = channelId
            self.participantRole = participantRole
        }
    }

}

extension TranscribeStreamingClientTypes {

    public enum ContentRedactionOutput: Swift.Equatable, Swift.RawRepresentable, Swift.CaseIterable, Swift.Hashable {
        case redacted
        case redactedAndUnredacted
        case sdkUnknown(Swift.String)

        public static var allCases: [ContentRedactionOutput] {
            return [
                .redacted,
                .redactedAndUnredacted
            ]
        }

        public init?(rawValue: Swift.String) {
            let value = Self.allCases.first(where: { $0.rawValue == rawValue })
            self = value ?? Self.sdkUnknown(rawValue)
        }

        public var rawValue: Swift.String {
            switch self {
            case .redacted: return "redacted"
            case .redactedAndUnredacted: return "redacted_and_unredacted"
            case let .sdkUnknown(s): return s
            }
        }
    }
}

extension TranscribeStreamingClientTypes {
    /// Allows you to specify additional settings for your streaming Call Analytics post-call request, including output locations for your redacted and unredacted transcript, which IAM role to use, and, optionally, which encryption key to use. ContentRedactionOutput, DataAccessRoleArn, and OutputLocation are required fields.
    public struct PostCallAnalyticsSettings {
        /// Specify whether you want only a redacted transcript or both a redacted and an unredacted transcript. If you choose redacted and unredacted, two JSON files are generated and stored in the Amazon S3 output location you specify. Note that to include ContentRedactionOutput in your request, you must enable content redaction (ContentRedactionType).
        public var contentRedactionOutput: TranscribeStreamingClientTypes.ContentRedactionOutput?
        /// The Amazon Resource Name (ARN) of an IAM role that has permissions to access the Amazon S3 bucket that contains your input files. If the role that you specify doesnâ€™t have the appropriate permissions to access the specified Amazon S3 location, your request fails. IAM role ARNs have the format arn:partition:iam::account:role/role-name-with-path. For example: arn:aws:iam::111122223333:role/Admin. For more information, see [IAM ARNs](https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_identifiers.html#identifiers-arns).
        /// This member is required.
        public var dataAccessRoleArn: Swift.String?
        /// The KMS key you want to use to encrypt your Call Analytics post-call output. If using a key located in the current Amazon Web Services account, you can specify your KMS key in one of four ways:
        ///
        /// * Use the KMS key ID itself. For example, 1234abcd-12ab-34cd-56ef-1234567890ab.
        ///
        /// * Use an alias for the KMS key ID. For example, alias/ExampleAlias.
        ///
        /// * Use the Amazon Resource Name (ARN) for the KMS key ID. For example, arn:aws:kms:region:account-ID:key/1234abcd-12ab-34cd-56ef-1234567890ab.
        ///
        /// * Use the ARN for the KMS key alias. For example, arn:aws:kms:region:account-ID:alias/ExampleAlias.
        ///
        ///
        /// If using a key located in a different Amazon Web Services account than the current Amazon Web Services account, you can specify your KMS key in one of two ways:
        ///
        /// * Use the ARN for the KMS key ID. For example, arn:aws:kms:region:account-ID:key/1234abcd-12ab-34cd-56ef-1234567890ab.
        ///
        /// * Use the ARN for the KMS key alias. For example, arn:aws:kms:region:account-ID:alias/ExampleAlias.
        ///
        ///
        /// Note that the user making the request must have permission to use the specified KMS key.
        public var outputEncryptionKMSKeyId: Swift.String?
        /// The Amazon S3 location where you want your Call Analytics post-call transcription output stored. You can use any of the following formats to specify the output location:
        ///
        /// * s3://DOC-EXAMPLE-BUCKET
        ///
        /// * s3://DOC-EXAMPLE-BUCKET/my-output-folder/
        ///
        /// * s3://DOC-EXAMPLE-BUCKET/my-output-folder/my-call-analytics-job.json
        /// This member is required.
        public var outputLocation: Swift.String?

        public init(
            contentRedactionOutput: TranscribeStreamingClientTypes.ContentRedactionOutput? = nil,
            dataAccessRoleArn: Swift.String? = nil,
            outputEncryptionKMSKeyId: Swift.String? = nil,
            outputLocation: Swift.String? = nil
        )
        {
            self.contentRedactionOutput = contentRedactionOutput
            self.dataAccessRoleArn = dataAccessRoleArn
            self.outputEncryptionKMSKeyId = outputEncryptionKMSKeyId
            self.outputLocation = outputLocation
        }
    }

}

extension TranscribeStreamingClientTypes {
    /// Allows you to set audio channel definitions and post-call analytics settings.
    public struct ConfigurationEvent {
        /// Indicates which speaker is on which audio channel.
        public var channelDefinitions: [TranscribeStreamingClientTypes.ChannelDefinition]?
        /// Provides additional optional settings for your Call Analytics post-call request, including encryption and output locations for your redacted and unredacted transcript.
        public var postCallAnalyticsSettings: TranscribeStreamingClientTypes.PostCallAnalyticsSettings?

        public init(
            channelDefinitions: [TranscribeStreamingClientTypes.ChannelDefinition]? = nil,
            postCallAnalyticsSettings: TranscribeStreamingClientTypes.PostCallAnalyticsSettings? = nil
        )
        {
            self.channelDefinitions = channelDefinitions
            self.postCallAnalyticsSettings = postCallAnalyticsSettings
        }
    }

}

extension TranscribeStreamingClientTypes {
    /// An encoded stream of audio blobs. Audio streams are encoded as either HTTP/2 or WebSocket data frames. For more information, see [Transcribing streaming audio](https://docs.aws.amazon.com/transcribe/latest/dg/streaming.html).
    public enum AudioStream {
        /// A blob of audio from your application. Your audio stream consists of one or more audio events. For more information, see [Event stream encoding](https://docs.aws.amazon.com/transcribe/latest/dg/event-stream.html).
        case audioevent(TranscribeStreamingClientTypes.AudioEvent)
        /// Contains audio channel definitions and post-call analytics settings.
        case configurationevent(TranscribeStreamingClientTypes.ConfigurationEvent)
        case sdkUnknown(Swift.String)
    }

}

/// One or more arguments to the StartStreamTranscription, StartMedicalStreamTranscription, or StartCallAnalyticsStreamTranscription operation was not valid. For example, MediaEncoding or LanguageCode used not valid values. Check the specified parameters and try your request again.
public struct BadRequestException: ClientRuntime.ModeledError, AWSClientRuntime.AWSServiceError, ClientRuntime.HTTPError, Swift.Error {

    public struct Properties {
        public internal(set) var message: Swift.String? = nil
    }

    public internal(set) var properties = Properties()
    public static var typeName: Swift.String { "BadRequestException" }
    public static var fault: ClientRuntime.ErrorFault { .client }
    public static var isRetryable: Swift.Bool { false }
    public static var isThrottling: Swift.Bool { false }
    public internal(set) var httpResponse = SmithyHTTPAPI.HTTPResponse()
    public internal(set) var message: Swift.String?
    public internal(set) var requestID: Swift.String?

    public init(
        message: Swift.String? = nil
    )
    {
        self.properties.message = message
    }
}

extension TranscribeStreamingClientTypes {
    /// Contains entities identified as personally identifiable information (PII) in your transcription output, along with various associated attributes. Examples include category, confidence score, content, type, and start and end times.
    public struct CallAnalyticsEntity {
        /// The time, in milliseconds, from the beginning of the audio stream to the start of the identified entity.
        public var beginOffsetMillis: Swift.Int?
        /// The category of information identified. For example, PII.
        public var category: Swift.String?
        /// The confidence score associated with the identification of an entity in your transcript. Confidence scores are values between 0 and 1. A larger value indicates a higher probability that the identified entity correctly matches the entity spoken in your media.
        public var confidence: Swift.Double?
        /// The word or words that represent the identified entity.
        public var content: Swift.String?
        /// The time, in milliseconds, from the beginning of the audio stream to the end of the identified entity.
        public var endOffsetMillis: Swift.Int?
        /// The type of PII identified. For example, NAME or CREDIT_DEBIT_NUMBER.
        public var type: Swift.String?

        public init(
            beginOffsetMillis: Swift.Int? = nil,
            category: Swift.String? = nil,
            confidence: Swift.Double? = nil,
            content: Swift.String? = nil,
            endOffsetMillis: Swift.Int? = nil,
            type: Swift.String? = nil
        )
        {
            self.beginOffsetMillis = beginOffsetMillis
            self.category = category
            self.confidence = confidence
            self.content = content
            self.endOffsetMillis = endOffsetMillis
            self.type = type
        }
    }

}

extension TranscribeStreamingClientTypes {
    /// A word, phrase, or punctuation mark in your Call Analytics transcription output, along with various associated attributes, such as confidence score, type, and start and end times.
    public struct CallAnalyticsItem {
        /// The time, in milliseconds, from the beginning of the audio stream to the start of the identified item.
        public var beginOffsetMillis: Swift.Int?
        /// The confidence score associated with a word or phrase in your transcript. Confidence scores are values between 0 and 1. A larger value indicates a higher probability that the identified item correctly matches the item spoken in your media.
        public var confidence: Swift.Double?
        /// The word or punctuation that was transcribed.
        public var content: Swift.String?
        /// The time, in milliseconds, from the beginning of the audio stream to the end of the identified item.
        public var endOffsetMillis: Swift.Int?
        /// If partial result stabilization is enabled, Stable indicates whether the specified item is stable (true) or if it may change when the segment is complete (false).
        public var stable: Swift.Bool?
        /// The type of item identified. Options are: PRONUNCIATION (spoken words) and PUNCTUATION.
        public var type: TranscribeStreamingClientTypes.ItemType?
        /// Indicates whether the specified item matches a word in the vocabulary filter included in your Call Analytics request. If true, there is a vocabulary filter match.
        public var vocabularyFilterMatch: Swift.Bool

        public init(
            beginOffsetMillis: Swift.Int? = nil,
            confidence: Swift.Double? = nil,
            content: Swift.String? = nil,
            endOffsetMillis: Swift.Int? = nil,
            stable: Swift.Bool? = nil,
            type: TranscribeStreamingClientTypes.ItemType? = nil,
            vocabularyFilterMatch: Swift.Bool = false
        )
        {
            self.beginOffsetMillis = beginOffsetMillis
            self.confidence = confidence
            self.content = content
            self.endOffsetMillis = endOffsetMillis
            self.stable = stable
            self.type = type
            self.vocabularyFilterMatch = vocabularyFilterMatch
        }
    }

}

extension TranscribeStreamingClientTypes {

    public enum CallAnalyticsLanguageCode: Swift.Equatable, Swift.RawRepresentable, Swift.CaseIterable, Swift.Hashable {
        case deDe
        case enAu
        case enGb
        case enUs
        case esUs
        case frCa
        case frFr
        case itIt
        case ptBr
        case sdkUnknown(Swift.String)

        public static var allCases: [CallAnalyticsLanguageCode] {
            return [
                .deDe,
                .enAu,
                .enGb,
                .enUs,
                .esUs,
                .frCa,
                .frFr,
                .itIt,
                .ptBr
            ]
        }

        public init?(rawValue: Swift.String) {
            let value = Self.allCases.first(where: { $0.rawValue == rawValue })
            self = value ?? Self.sdkUnknown(rawValue)
        }

        public var rawValue: Swift.String {
            switch self {
            case .deDe: return "de-DE"
            case .enAu: return "en-AU"
            case .enGb: return "en-GB"
            case .enUs: return "en-US"
            case .esUs: return "es-US"
            case .frCa: return "fr-CA"
            case .frFr: return "fr-FR"
            case .itIt: return "it-IT"
            case .ptBr: return "pt-BR"
            case let .sdkUnknown(s): return s
            }
        }
    }
}

extension TranscribeStreamingClientTypes {
    /// Contains the timestamp range (start time through end time) of a matched category.
    public struct TimestampRange {
        /// The time, in milliseconds, from the beginning of the audio stream to the start of the category match.
        public var beginOffsetMillis: Swift.Int?
        /// The time, in milliseconds, from the beginning of the audio stream to the end of the category match.
        public var endOffsetMillis: Swift.Int?

        public init(
            beginOffsetMillis: Swift.Int? = nil,
            endOffsetMillis: Swift.Int? = nil
        )
        {
            self.beginOffsetMillis = beginOffsetMillis
            self.endOffsetMillis = endOffsetMillis
        }
    }

}

extension TranscribeStreamingClientTypes {
    /// Contains the timestamps of matched categories.
    public struct PointsOfInterest {
        /// Contains the timestamp ranges (start time through end time) of matched categories and rules.
        public var timestampRanges: [TranscribeStreamingClientTypes.TimestampRange]?

        public init(
            timestampRanges: [TranscribeStreamingClientTypes.TimestampRange]? = nil
        )
        {
            self.timestampRanges = timestampRanges
        }
    }

}

extension TranscribeStreamingClientTypes {
    /// Provides information on any TranscriptFilterType categories that matched your transcription output. Matches are identified for each segment upon completion of that segment.
    public struct CategoryEvent {
        /// Lists the categories that were matched in your audio segment.
        public var matchedCategories: [Swift.String]?
        /// Contains information about the matched categories, including category names and timestamps.
        public var matchedDetails: [Swift.String: TranscribeStreamingClientTypes.PointsOfInterest]?

        public init(
            matchedCategories: [Swift.String]? = nil,
            matchedDetails: [Swift.String: TranscribeStreamingClientTypes.PointsOfInterest]? = nil
        )
        {
            self.matchedCategories = matchedCategories
            self.matchedDetails = matchedDetails
        }
    }

}

/// A new stream started with the same session ID. The current stream has been terminated.
public struct ConflictException: ClientRuntime.ModeledError, AWSClientRuntime.AWSServiceError, ClientRuntime.HTTPError, Swift.Error {

    public struct Properties {
        public internal(set) var message: Swift.String? = nil
    }

    public internal(set) var properties = Properties()
    public static var typeName: Swift.String { "ConflictException" }
    public static var fault: ClientRuntime.ErrorFault { .client }
    public static var isRetryable: Swift.Bool { false }
    public static var isThrottling: Swift.Bool { false }
    public internal(set) var httpResponse = SmithyHTTPAPI.HTTPResponse()
    public internal(set) var message: Swift.String?
    public internal(set) var requestID: Swift.String?

    public init(
        message: Swift.String? = nil
    )
    {
        self.properties.message = message
    }
}

/// A problem occurred while processing the audio. Amazon Transcribe terminated processing.
public struct InternalFailureException: ClientRuntime.ModeledError, AWSClientRuntime.AWSServiceError, ClientRuntime.HTTPError, Swift.Error {

    public struct Properties {
        public internal(set) var message: Swift.String? = nil
    }

    public internal(set) var properties = Properties()
    public static var typeName: Swift.String { "InternalFailureException" }
    public static var fault: ClientRuntime.ErrorFault { .server }
    public static var isRetryable: Swift.Bool { false }
    public static var isThrottling: Swift.Bool { false }
    public internal(set) var httpResponse = SmithyHTTPAPI.HTTPResponse()
    public internal(set) var message: Swift.String?
    public internal(set) var requestID: Swift.String?

    public init(
        message: Swift.String? = nil
    )
    {
        self.properties.message = message
    }
}

/// Your client has exceeded one of the Amazon Transcribe limits. This is typically the audio length limit. Break your audio stream into smaller chunks and try your request again.
public struct LimitExceededException: ClientRuntime.ModeledError, AWSClientRuntime.AWSServiceError, ClientRuntime.HTTPError, Swift.Error {

    public struct Properties {
        public internal(set) var message: Swift.String? = nil
    }

    public internal(set) var properties = Properties()
    public static var typeName: Swift.String { "LimitExceededException" }
    public static var fault: ClientRuntime.ErrorFault { .client }
    public static var isRetryable: Swift.Bool { false }
    public static var isThrottling: Swift.Bool { false }
    public internal(set) var httpResponse = SmithyHTTPAPI.HTTPResponse()
    public internal(set) var message: Swift.String?
    public internal(set) var requestID: Swift.String?

    public init(
        message: Swift.String? = nil
    )
    {
        self.properties.message = message
    }
}

/// The service is currently unavailable. Try your request later.
public struct ServiceUnavailableException: ClientRuntime.ModeledError, AWSClientRuntime.AWSServiceError, ClientRuntime.HTTPError, Swift.Error {

    public struct Properties {
        public internal(set) var message: Swift.String? = nil
    }

    public internal(set) var properties = Properties()
    public static var typeName: Swift.String { "ServiceUnavailableException" }
    public static var fault: ClientRuntime.ErrorFault { .server }
    public static var isRetryable: Swift.Bool { false }
    public static var isThrottling: Swift.Bool { false }
    public internal(set) var httpResponse = SmithyHTTPAPI.HTTPResponse()
    public internal(set) var message: Swift.String?
    public internal(set) var requestID: Swift.String?

    public init(
        message: Swift.String? = nil
    )
    {
        self.properties.message = message
    }
}

extension TranscribeStreamingClientTypes {
    /// Provides the location, using character count, in your transcript where a match is identified. For example, the location of an issue or a category match within a segment.
    public struct CharacterOffsets {
        /// Provides the character count of the first character where a match is identified. For example, the first character associated with an issue or a category match in a segment transcript.
        public var begin: Swift.Int?
        /// Provides the character count of the last character where a match is identified. For example, the last character associated with an issue or a category match in a segment transcript.
        public var end: Swift.Int?

        public init(
            begin: Swift.Int? = nil,
            end: Swift.Int? = nil
        )
        {
            self.begin = begin
            self.end = end
        }
    }

}

extension TranscribeStreamingClientTypes {
    /// Lists the issues that were identified in your audio segment.
    public struct IssueDetected {
        /// Provides the timestamps that identify when in an audio segment the specified issue occurs.
        public var characterOffsets: TranscribeStreamingClientTypes.CharacterOffsets?

        public init(
            characterOffsets: TranscribeStreamingClientTypes.CharacterOffsets? = nil
        )
        {
            self.characterOffsets = characterOffsets
        }
    }

}

extension TranscribeStreamingClientTypes {

    public enum Sentiment: Swift.Equatable, Swift.RawRepresentable, Swift.CaseIterable, Swift.Hashable {
        case mixed
        case negative
        case neutral
        case positive
        case sdkUnknown(Swift.String)

        public static var allCases: [Sentiment] {
            return [
                .mixed,
                .negative,
                .neutral,
                .positive
            ]
        }

        public init?(rawValue: Swift.String) {
            let value = Self.allCases.first(where: { $0.rawValue == rawValue })
            self = value ?? Self.sdkUnknown(rawValue)
        }

        public var rawValue: Swift.String {
            switch self {
            case .mixed: return "MIXED"
            case .negative: return "NEGATIVE"
            case .neutral: return "NEUTRAL"
            case .positive: return "POSITIVE"
            case let .sdkUnknown(s): return s
            }
        }
    }
}

extension TranscribeStreamingClientTypes {
    /// Contains set of transcription results from one or more audio segments, along with additional information about the parameters included in your request. For example, channel definitions, partial result stabilization, sentiment, and issue detection.
    public struct UtteranceEvent {
        /// The time, in milliseconds, from the beginning of the audio stream to the start of the UtteranceEvent.
        public var beginOffsetMillis: Swift.Int?
        /// The time, in milliseconds, from the beginning of the audio stream to the start of the UtteranceEvent.
        public var endOffsetMillis: Swift.Int?
        /// Contains entities identified as personally identifiable information (PII) in your transcription output.
        public var entities: [TranscribeStreamingClientTypes.CallAnalyticsEntity]?
        /// Indicates whether the segment in the UtteranceEvent is complete (FALSE) or partial (TRUE).
        public var isPartial: Swift.Bool
        /// Provides the issue that was detected in the specified segment.
        public var issuesDetected: [TranscribeStreamingClientTypes.IssueDetected]?
        /// Contains words, phrases, or punctuation marks that are associated with the specified UtteranceEvent.
        public var items: [TranscribeStreamingClientTypes.CallAnalyticsItem]?
        /// Provides the role of the speaker for each audio channel, either CUSTOMER or AGENT.
        public var participantRole: TranscribeStreamingClientTypes.ParticipantRole?
        /// Provides the sentiment that was detected in the specified segment.
        public var sentiment: TranscribeStreamingClientTypes.Sentiment?
        /// Contains transcribed text.
        public var transcript: Swift.String?
        /// The unique identifier that is associated with the specified UtteranceEvent.
        public var utteranceId: Swift.String?

        public init(
            beginOffsetMillis: Swift.Int? = nil,
            endOffsetMillis: Swift.Int? = nil,
            entities: [TranscribeStreamingClientTypes.CallAnalyticsEntity]? = nil,
            isPartial: Swift.Bool = false,
            issuesDetected: [TranscribeStreamingClientTypes.IssueDetected]? = nil,
            items: [TranscribeStreamingClientTypes.CallAnalyticsItem]? = nil,
            participantRole: TranscribeStreamingClientTypes.ParticipantRole? = nil,
            sentiment: TranscribeStreamingClientTypes.Sentiment? = nil,
            transcript: Swift.String? = nil,
            utteranceId: Swift.String? = nil
        )
        {
            self.beginOffsetMillis = beginOffsetMillis
            self.endOffsetMillis = endOffsetMillis
            self.entities = entities
            self.isPartial = isPartial
            self.issuesDetected = issuesDetected
            self.items = items
            self.participantRole = participantRole
            self.sentiment = sentiment
            self.transcript = transcript
            self.utteranceId = utteranceId
        }
    }

}

extension TranscribeStreamingClientTypes {
    /// Contains detailed information about your Call Analytics streaming session. These details are provided in the UtteranceEvent and CategoryEvent objects.
    public enum CallAnalyticsTranscriptResultStream {
        /// Contains set of transcription results from one or more audio segments, along with additional information per your request parameters. This can include information relating to channel definitions, partial result stabilization, sentiment, issue detection, and other transcription-related data.
        case utteranceevent(TranscribeStreamingClientTypes.UtteranceEvent)
        /// Provides information on matched categories that were used to generate real-time supervisor alerts.
        case categoryevent(TranscribeStreamingClientTypes.CategoryEvent)
        case sdkUnknown(Swift.String)
    }

}

extension TranscribeStreamingClientTypes {

    public enum ContentIdentificationType: Swift.Equatable, Swift.RawRepresentable, Swift.CaseIterable, Swift.Hashable {
        case pii
        case sdkUnknown(Swift.String)

        public static var allCases: [ContentIdentificationType] {
            return [
                .pii
            ]
        }

        public init?(rawValue: Swift.String) {
            let value = Self.allCases.first(where: { $0.rawValue == rawValue })
            self = value ?? Self.sdkUnknown(rawValue)
        }

        public var rawValue: Swift.String {
            switch self {
            case .pii: return "PII"
            case let .sdkUnknown(s): return s
            }
        }
    }
}

extension TranscribeStreamingClientTypes {

    public enum ContentRedactionType: Swift.Equatable, Swift.RawRepresentable, Swift.CaseIterable, Swift.Hashable {
        case pii
        case sdkUnknown(Swift.String)

        public static var allCases: [ContentRedactionType] {
            return [
                .pii
            ]
        }

        public init?(rawValue: Swift.String) {
            let value = Self.allCases.first(where: { $0.rawValue == rawValue })
            self = value ?? Self.sdkUnknown(rawValue)
        }

        public var rawValue: Swift.String {
            switch self {
            case .pii: return "PII"
            case let .sdkUnknown(s): return s
            }
        }
    }
}

extension TranscribeStreamingClientTypes {

    public enum LanguageCode: Swift.Equatable, Swift.RawRepresentable, Swift.CaseIterable, Swift.Hashable {
        case deDe
        case enAu
        case enGb
        case enUs
        case esUs
        case frCa
        case frFr
        case hiIn
        case itIt
        case jaJp
        case koKr
        case ptBr
        case thTh
        case zhCn
        case sdkUnknown(Swift.String)

        public static var allCases: [LanguageCode] {
            return [
                .deDe,
                .enAu,
                .enGb,
                .enUs,
                .esUs,
                .frCa,
                .frFr,
                .hiIn,
                .itIt,
                .jaJp,
                .koKr,
                .ptBr,
                .thTh,
                .zhCn
            ]
        }

        public init?(rawValue: Swift.String) {
            let value = Self.allCases.first(where: { $0.rawValue == rawValue })
            self = value ?? Self.sdkUnknown(rawValue)
        }

        public var rawValue: Swift.String {
            switch self {
            case .deDe: return "de-DE"
            case .enAu: return "en-AU"
            case .enGb: return "en-GB"
            case .enUs: return "en-US"
            case .esUs: return "es-US"
            case .frCa: return "fr-CA"
            case .frFr: return "fr-FR"
            case .hiIn: return "hi-IN"
            case .itIt: return "it-IT"
            case .jaJp: return "ja-JP"
            case .koKr: return "ko-KR"
            case .ptBr: return "pt-BR"
            case .thTh: return "th-TH"
            case .zhCn: return "zh-CN"
            case let .sdkUnknown(s): return s
            }
        }
    }
}

extension TranscribeStreamingClientTypes {
    /// The language code that represents the language identified in your audio, including the associated confidence score. If you enabled channel identification in your request and each channel contained a different language, you will have more than one LanguageWithScore result.
    public struct LanguageWithScore {
        /// The language code of the identified language.
        public var languageCode: TranscribeStreamingClientTypes.LanguageCode?
        /// The confidence score associated with the identified language code. Confidence scores are values between zero and one; larger values indicate a higher confidence in the identified language.
        public var score: Swift.Double

        public init(
            languageCode: TranscribeStreamingClientTypes.LanguageCode? = nil,
            score: Swift.Double = 0.0
        )
        {
            self.languageCode = languageCode
            self.score = score
        }
    }

}

extension TranscribeStreamingClientTypes {

    public enum MediaEncoding: Swift.Equatable, Swift.RawRepresentable, Swift.CaseIterable, Swift.Hashable {
        case flac
        case oggOpus
        case pcm
        case sdkUnknown(Swift.String)

        public static var allCases: [MediaEncoding] {
            return [
                .flac,
                .oggOpus,
                .pcm
            ]
        }

        public init?(rawValue: Swift.String) {
            let value = Self.allCases.first(where: { $0.rawValue == rawValue })
            self = value ?? Self.sdkUnknown(rawValue)
        }

        public var rawValue: Swift.String {
            switch self {
            case .flac: return "flac"
            case .oggOpus: return "ogg-opus"
            case .pcm: return "pcm"
            case let .sdkUnknown(s): return s
            }
        }
    }
}

extension TranscribeStreamingClientTypes {
    /// Contains entities identified as personal health information (PHI) in your transcription output, along with various associated attributes. Examples include category, confidence score, type, stability score, and start and end times.
    public struct MedicalEntity {
        /// The category of information identified. The only category is PHI.
        public var category: Swift.String?
        /// The confidence score associated with the identified PHI entity in your audio. Confidence scores are values between 0 and 1. A larger value indicates a higher probability that the identified entity correctly matches the entity spoken in your media.
        public var confidence: Swift.Double?
        /// The word or words identified as PHI.
        public var content: Swift.String?
        /// The end time, in milliseconds, of the utterance that was identified as PHI.
        public var endTime: Swift.Double
        /// The start time, in milliseconds, of the utterance that was identified as PHI.
        public var startTime: Swift.Double

        public init(
            category: Swift.String? = nil,
            confidence: Swift.Double? = nil,
            content: Swift.String? = nil,
            endTime: Swift.Double = 0.0,
            startTime: Swift.Double = 0.0
        )
        {
            self.category = category
            self.confidence = confidence
            self.content = content
            self.endTime = endTime
            self.startTime = startTime
        }
    }

}

extension TranscribeStreamingClientTypes {
    /// A word, phrase, or punctuation mark in your transcription output, along with various associated attributes, such as confidence score, type, and start and end times.
    public struct MedicalItem {
        /// The confidence score associated with a word or phrase in your transcript. Confidence scores are values between 0 and 1. A larger value indicates a higher probability that the identified item correctly matches the item spoken in your media.
        public var confidence: Swift.Double?
        /// The word or punctuation that was transcribed.
        public var content: Swift.String?
        /// The end time, in milliseconds, of the transcribed item.
        public var endTime: Swift.Double
        /// If speaker partitioning is enabled, Speaker labels the speaker of the specified item.
        public var speaker: Swift.String?
        /// The start time, in milliseconds, of the transcribed item.
        public var startTime: Swift.Double
        /// The type of item identified. Options are: PRONUNCIATION (spoken words) and PUNCTUATION.
        public var type: TranscribeStreamingClientTypes.ItemType?

        public init(
            confidence: Swift.Double? = nil,
            content: Swift.String? = nil,
            endTime: Swift.Double = 0.0,
            speaker: Swift.String? = nil,
            startTime: Swift.Double = 0.0,
            type: TranscribeStreamingClientTypes.ItemType? = nil
        )
        {
            self.confidence = confidence
            self.content = content
            self.endTime = endTime
            self.speaker = speaker
            self.startTime = startTime
            self.type = type
        }
    }

}

extension TranscribeStreamingClientTypes {
    /// A list of possible alternative transcriptions for the input audio. Each alternative may contain one or more of Items, Entities, or Transcript.
    public struct MedicalAlternative {
        /// Contains entities identified as personal health information (PHI) in your transcription output.
        public var entities: [TranscribeStreamingClientTypes.MedicalEntity]?
        /// Contains words, phrases, or punctuation marks in your transcription output.
        public var items: [TranscribeStreamingClientTypes.MedicalItem]?
        /// Contains transcribed text.
        public var transcript: Swift.String?

        public init(
            entities: [TranscribeStreamingClientTypes.MedicalEntity]? = nil,
            items: [TranscribeStreamingClientTypes.MedicalItem]? = nil,
            transcript: Swift.String? = nil
        )
        {
            self.entities = entities
            self.items = items
            self.transcript = transcript
        }
    }

}

extension TranscribeStreamingClientTypes {

    public enum MedicalContentIdentificationType: Swift.Equatable, Swift.RawRepresentable, Swift.CaseIterable, Swift.Hashable {
        case phi
        case sdkUnknown(Swift.String)

        public static var allCases: [MedicalContentIdentificationType] {
            return [
                .phi
            ]
        }

        public init?(rawValue: Swift.String) {
            let value = Self.allCases.first(where: { $0.rawValue == rawValue })
            self = value ?? Self.sdkUnknown(rawValue)
        }

        public var rawValue: Swift.String {
            switch self {
            case .phi: return "PHI"
            case let .sdkUnknown(s): return s
            }
        }
    }
}

extension TranscribeStreamingClientTypes {
    /// The Result associated with a . Contains a set of transcription results from one or more audio segments, along with additional information per your request parameters. This can include information relating to alternative transcriptions, channel identification, partial result stabilization, language identification, and other transcription-related data.
    public struct MedicalResult {
        /// A list of possible alternative transcriptions for the input audio. Each alternative may contain one or more of Items, Entities, or Transcript.
        public var alternatives: [TranscribeStreamingClientTypes.MedicalAlternative]?
        /// Indicates the channel identified for the Result.
        public var channelId: Swift.String?
        /// The end time, in milliseconds, of the Result.
        public var endTime: Swift.Double
        /// Indicates if the segment is complete. If IsPartial is true, the segment is not complete. If IsPartial is false, the segment is complete.
        public var isPartial: Swift.Bool
        /// Provides a unique identifier for the Result.
        public var resultId: Swift.String?
        /// The start time, in milliseconds, of the Result.
        public var startTime: Swift.Double

        public init(
            alternatives: [TranscribeStreamingClientTypes.MedicalAlternative]? = nil,
            channelId: Swift.String? = nil,
            endTime: Swift.Double = 0.0,
            isPartial: Swift.Bool = false,
            resultId: Swift.String? = nil,
            startTime: Swift.Double = 0.0
        )
        {
            self.alternatives = alternatives
            self.channelId = channelId
            self.endTime = endTime
            self.isPartial = isPartial
            self.resultId = resultId
            self.startTime = startTime
        }
    }

}

extension TranscribeStreamingClientTypes {
    /// The MedicalTranscript associated with a . MedicalTranscript contains Results, which contains a set of transcription results from one or more audio segments, along with additional information per your request parameters.
    public struct MedicalTranscript {
        /// Contains a set of transcription results from one or more audio segments, along with additional information per your request parameters. This can include information relating to alternative transcriptions, channel identification, partial result stabilization, language identification, and other transcription-related data.
        public var results: [TranscribeStreamingClientTypes.MedicalResult]?

        public init(
            results: [TranscribeStreamingClientTypes.MedicalResult]? = nil
        )
        {
            self.results = results
        }
    }

}

extension TranscribeStreamingClientTypes {
    /// The MedicalTranscriptEvent associated with a MedicalTranscriptResultStream. Contains a set of transcription results from one or more audio segments, along with additional information per your request parameters.
    public struct MedicalTranscriptEvent {
        /// Contains Results, which contains a set of transcription results from one or more audio segments, along with additional information per your request parameters. This can include information relating to alternative transcriptions, channel identification, partial result stabilization, language identification, and other transcription-related data.
        public var transcript: TranscribeStreamingClientTypes.MedicalTranscript?

        public init(
            transcript: TranscribeStreamingClientTypes.MedicalTranscript? = nil
        )
        {
            self.transcript = transcript
        }
    }

}

extension TranscribeStreamingClientTypes {
    /// Contains detailed information about your streaming session.
    public enum MedicalTranscriptResultStream {
        /// The MedicalTranscriptEvent associated with a MedicalTranscriptResultStream. Contains a set of transcription results from one or more audio segments, along with additional information per your request parameters. This can include information relating to alternative transcriptions, channel identification, partial result stabilization, language identification, and other transcription-related data.
        case transcriptevent(TranscribeStreamingClientTypes.MedicalTranscriptEvent)
        case sdkUnknown(Swift.String)
    }

}

extension TranscribeStreamingClientTypes {

    public enum PartialResultsStability: Swift.Equatable, Swift.RawRepresentable, Swift.CaseIterable, Swift.Hashable {
        case high
        case low
        case medium
        case sdkUnknown(Swift.String)

        public static var allCases: [PartialResultsStability] {
            return [
                .high,
                .low,
                .medium
            ]
        }

        public init?(rawValue: Swift.String) {
            let value = Self.allCases.first(where: { $0.rawValue == rawValue })
            self = value ?? Self.sdkUnknown(rawValue)
        }

        public var rawValue: Swift.String {
            switch self {
            case .high: return "high"
            case .low: return "low"
            case .medium: return "medium"
            case let .sdkUnknown(s): return s
            }
        }
    }
}

extension TranscribeStreamingClientTypes {
    /// The Result associated with a . Contains a set of transcription results from one or more audio segments, along with additional information per your request parameters. This can include information relating to alternative transcriptions, channel identification, partial result stabilization, language identification, and other transcription-related data.
    public struct Result {
        /// A list of possible alternative transcriptions for the input audio. Each alternative may contain one or more of Items, Entities, or Transcript.
        public var alternatives: [TranscribeStreamingClientTypes.Alternative]?
        /// Indicates which audio channel is associated with the Result.
        public var channelId: Swift.String?
        /// The end time, in milliseconds, of the Result.
        public var endTime: Swift.Double
        /// Indicates if the segment is complete. If IsPartial is true, the segment is not complete. If IsPartial is false, the segment is complete.
        public var isPartial: Swift.Bool
        /// The language code that represents the language spoken in your audio stream.
        public var languageCode: TranscribeStreamingClientTypes.LanguageCode?
        /// The language code of the dominant language identified in your stream. If you enabled channel identification and each channel of your audio contains a different language, you may have more than one result.
        public var languageIdentification: [TranscribeStreamingClientTypes.LanguageWithScore]?
        /// Provides a unique identifier for the Result.
        public var resultId: Swift.String?
        /// The start time, in milliseconds, of the Result.
        public var startTime: Swift.Double

        public init(
            alternatives: [TranscribeStreamingClientTypes.Alternative]? = nil,
            channelId: Swift.String? = nil,
            endTime: Swift.Double = 0.0,
            isPartial: Swift.Bool = false,
            languageCode: TranscribeStreamingClientTypes.LanguageCode? = nil,
            languageIdentification: [TranscribeStreamingClientTypes.LanguageWithScore]? = nil,
            resultId: Swift.String? = nil,
            startTime: Swift.Double = 0.0
        )
        {
            self.alternatives = alternatives
            self.channelId = channelId
            self.endTime = endTime
            self.isPartial = isPartial
            self.languageCode = languageCode
            self.languageIdentification = languageIdentification
            self.resultId = resultId
            self.startTime = startTime
        }
    }

}

extension TranscribeStreamingClientTypes {

    public enum Specialty: Swift.Equatable, Swift.RawRepresentable, Swift.CaseIterable, Swift.Hashable {
        case cardiology
        case neurology
        case oncology
        case primarycare
        case radiology
        case urology
        case sdkUnknown(Swift.String)

        public static var allCases: [Specialty] {
            return [
                .cardiology,
                .neurology,
                .oncology,
                .primarycare,
                .radiology,
                .urology
            ]
        }

        public init?(rawValue: Swift.String) {
            let value = Self.allCases.first(where: { $0.rawValue == rawValue })
            self = value ?? Self.sdkUnknown(rawValue)
        }

        public var rawValue: Swift.String {
            switch self {
            case .cardiology: return "CARDIOLOGY"
            case .neurology: return "NEUROLOGY"
            case .oncology: return "ONCOLOGY"
            case .primarycare: return "PRIMARYCARE"
            case .radiology: return "RADIOLOGY"
            case .urology: return "UROLOGY"
            case let .sdkUnknown(s): return s
            }
        }
    }
}

extension TranscribeStreamingClientTypes {

    public enum VocabularyFilterMethod: Swift.Equatable, Swift.RawRepresentable, Swift.CaseIterable, Swift.Hashable {
        case mask
        case remove
        case tag
        case sdkUnknown(Swift.String)

        public static var allCases: [VocabularyFilterMethod] {
            return [
                .mask,
                .remove,
                .tag
            ]
        }

        public init?(rawValue: Swift.String) {
            let value = Self.allCases.first(where: { $0.rawValue == rawValue })
            self = value ?? Self.sdkUnknown(rawValue)
        }

        public var rawValue: Swift.String {
            switch self {
            case .mask: return "mask"
            case .remove: return "remove"
            case .tag: return "tag"
            case let .sdkUnknown(s): return s
            }
        }
    }
}

public struct StartCallAnalyticsStreamTranscriptionInput {
    /// An encoded stream of audio blobs. Audio streams are encoded as either HTTP/2 or WebSocket data frames. For more information, see [Transcribing streaming audio](https://docs.aws.amazon.com/transcribe/latest/dg/streaming.html).
    /// This member is required.
    public var audioStream: AsyncThrowingStream<TranscribeStreamingClientTypes.AudioStream, Swift.Error>?
    /// Labels all personally identifiable information (PII) identified in your transcript. Content identification is performed at the segment level; PII specified in PiiEntityTypes is flagged upon complete transcription of an audio segment. You canâ€™t set ContentIdentificationType and ContentRedactionType in the same request. If you set both, your request returns a BadRequestException. For more information, see [Redacting or identifying personally identifiable information](https://docs.aws.amazon.com/transcribe/latest/dg/pii-redaction.html).
    public var contentIdentificationType: TranscribeStreamingClientTypes.ContentIdentificationType?
    /// Redacts all personally identifiable information (PII) identified in your transcript. Content redaction is performed at the segment level; PII specified in PiiEntityTypes is redacted upon complete transcription of an audio segment. You canâ€™t set ContentRedactionType and ContentIdentificationType in the same request. If you set both, your request returns a BadRequestException. For more information, see [Redacting or identifying personally identifiable information](https://docs.aws.amazon.com/transcribe/latest/dg/pii-redaction.html).
    public var contentRedactionType: TranscribeStreamingClientTypes.ContentRedactionType?
    /// Enables partial result stabilization for your transcription. Partial result stabilization can reduce latency in your output, but may impact accuracy. For more information, see [Partial-result stabilization](https://docs.aws.amazon.com/transcribe/latest/dg/streaming.html#streaming-partial-result-stabilization).
    public var enablePartialResultsStabilization: Swift.Bool?
    /// Specify the language code that represents the language spoken in your audio. If you're unsure of the language spoken in your audio, consider using IdentifyLanguage to enable automatic language identification. For a list of languages supported with streaming Call Analytics, refer to the [Supported languages](https://docs.aws.amazon.com/transcribe/latest/dg/supported-languages.html) table.
    /// This member is required.
    public var languageCode: TranscribeStreamingClientTypes.CallAnalyticsLanguageCode?
    /// Specify the name of the custom language model that you want to use when processing your transcription. Note that language model names are case sensitive. The language of the specified language model must match the language code you specify in your transcription request. If the languages don't match, the custom language model isn't applied. There are no errors or warnings associated with a language mismatch. For more information, see [Custom language models](https://docs.aws.amazon.com/transcribe/latest/dg/custom-language-models.html).
    public var languageModelName: Swift.String?
    /// Specify the encoding of your input audio. Supported formats are:
    ///
    /// * FLAC
    ///
    /// * OPUS-encoded audio in an Ogg container
    ///
    /// * PCM (only signed 16-bit little-endian audio formats, which does not include WAV)
    ///
    ///
    /// For more information, see [Media formats](https://docs.aws.amazon.com/transcribe/latest/dg/how-input.html#how-input-audio).
    /// This member is required.
    public var mediaEncoding: TranscribeStreamingClientTypes.MediaEncoding?
    /// The sample rate of the input audio (in hertz). Low-quality audio, such as telephone audio, is typically around 8,000 Hz. High-quality audio typically ranges from 16,000 Hz to 48,000 Hz. Note that the sample rate you specify must match that of your audio.
    /// This member is required.
    public var mediaSampleRateHertz: Swift.Int?
    /// Specify the level of stability to use when you enable partial results stabilization (EnablePartialResultsStabilization). Low stability provides the highest accuracy. High stability transcribes faster, but with slightly lower accuracy. For more information, see [Partial-result stabilization](https://docs.aws.amazon.com/transcribe/latest/dg/streaming.html#streaming-partial-result-stabilization).
    public var partialResultsStability: TranscribeStreamingClientTypes.PartialResultsStability?
    /// Specify which types of personally identifiable information (PII) you want to redact in your transcript. You can include as many types as you'd like, or you can select ALL. To include PiiEntityTypes in your Call Analytics request, you must also include either ContentIdentificationType or ContentRedactionType. Values must be comma-separated and can include: BANK_ACCOUNT_NUMBER, BANK_ROUTING, CREDIT_DEBIT_NUMBER, CREDIT_DEBIT_CVV, CREDIT_DEBIT_EXPIRY, PIN, EMAIL, ADDRESS, NAME, PHONE, SSN, or ALL.
    public var piiEntityTypes: Swift.String?
    /// Specify a name for your Call Analytics transcription session. If you don't include this parameter in your request, Amazon Transcribe generates an ID and returns it in the response. You can use a session ID to retry a streaming session.
    public var sessionId: Swift.String?
    /// Specify how you want your vocabulary filter applied to your transcript. To replace words with ***, choose mask. To delete words, choose remove. To flag words without changing them, choose tag.
    public var vocabularyFilterMethod: TranscribeStreamingClientTypes.VocabularyFilterMethod?
    /// Specify the name of the custom vocabulary filter that you want to use when processing your transcription. Note that vocabulary filter names are case sensitive. If the language of the specified custom vocabulary filter doesn't match the language identified in your media, the vocabulary filter is not applied to your transcription. For more information, see [Using vocabulary filtering with unwanted words](https://docs.aws.amazon.com/transcribe/latest/dg/vocabulary-filtering.html).
    public var vocabularyFilterName: Swift.String?
    /// Specify the name of the custom vocabulary that you want to use when processing your transcription. Note that vocabulary names are case sensitive. If the language of the specified custom vocabulary doesn't match the language identified in your media, the custom vocabulary is not applied to your transcription. For more information, see [Custom vocabularies](https://docs.aws.amazon.com/transcribe/latest/dg/custom-vocabulary.html).
    public var vocabularyName: Swift.String?

    public init(
        audioStream: AsyncThrowingStream<TranscribeStreamingClientTypes.AudioStream, Swift.Error>? = nil,
        contentIdentificationType: TranscribeStreamingClientTypes.ContentIdentificationType? = nil,
        contentRedactionType: TranscribeStreamingClientTypes.ContentRedactionType? = nil,
        enablePartialResultsStabilization: Swift.Bool? = false,
        languageCode: TranscribeStreamingClientTypes.CallAnalyticsLanguageCode? = nil,
        languageModelName: Swift.String? = nil,
        mediaEncoding: TranscribeStreamingClientTypes.MediaEncoding? = nil,
        mediaSampleRateHertz: Swift.Int? = nil,
        partialResultsStability: TranscribeStreamingClientTypes.PartialResultsStability? = nil,
        piiEntityTypes: Swift.String? = nil,
        sessionId: Swift.String? = nil,
        vocabularyFilterMethod: TranscribeStreamingClientTypes.VocabularyFilterMethod? = nil,
        vocabularyFilterName: Swift.String? = nil,
        vocabularyName: Swift.String? = nil
    )
    {
        self.audioStream = audioStream
        self.contentIdentificationType = contentIdentificationType
        self.contentRedactionType = contentRedactionType
        self.enablePartialResultsStabilization = enablePartialResultsStabilization
        self.languageCode = languageCode
        self.languageModelName = languageModelName
        self.mediaEncoding = mediaEncoding
        self.mediaSampleRateHertz = mediaSampleRateHertz
        self.partialResultsStability = partialResultsStability
        self.piiEntityTypes = piiEntityTypes
        self.sessionId = sessionId
        self.vocabularyFilterMethod = vocabularyFilterMethod
        self.vocabularyFilterName = vocabularyFilterName
        self.vocabularyName = vocabularyName
    }
}

public struct StartCallAnalyticsStreamTranscriptionOutput {
    /// Provides detailed information about your Call Analytics streaming session.
    public var callAnalyticsTranscriptResultStream: AsyncThrowingStream<TranscribeStreamingClientTypes.CallAnalyticsTranscriptResultStream, Swift.Error>?
    /// Shows whether content identification was enabled for your Call Analytics transcription.
    public var contentIdentificationType: TranscribeStreamingClientTypes.ContentIdentificationType?
    /// Shows whether content redaction was enabled for your Call Analytics transcription.
    public var contentRedactionType: TranscribeStreamingClientTypes.ContentRedactionType?
    /// Shows whether partial results stabilization was enabled for your Call Analytics transcription.
    public var enablePartialResultsStabilization: Swift.Bool
    /// Provides the language code that you specified in your Call Analytics request.
    public var languageCode: TranscribeStreamingClientTypes.CallAnalyticsLanguageCode?
    /// Provides the name of the custom language model that you specified in your Call Analytics request.
    public var languageModelName: Swift.String?
    /// Provides the media encoding you specified in your Call Analytics request.
    public var mediaEncoding: TranscribeStreamingClientTypes.MediaEncoding?
    /// Provides the sample rate that you specified in your Call Analytics request.
    public var mediaSampleRateHertz: Swift.Int?
    /// Provides the stabilization level used for your transcription.
    public var partialResultsStability: TranscribeStreamingClientTypes.PartialResultsStability?
    /// Lists the PII entity types you specified in your Call Analytics request.
    public var piiEntityTypes: Swift.String?
    /// Provides the identifier for your Call Analytics streaming request.
    public var requestId: Swift.String?
    /// Provides the identifier for your Call Analytics transcription session.
    public var sessionId: Swift.String?
    /// Provides the vocabulary filtering method used in your Call Analytics transcription.
    public var vocabularyFilterMethod: TranscribeStreamingClientTypes.VocabularyFilterMethod?
    /// Provides the name of the custom vocabulary filter that you specified in your Call Analytics request.
    public var vocabularyFilterName: Swift.String?
    /// Provides the name of the custom vocabulary that you specified in your Call Analytics request.
    public var vocabularyName: Swift.String?

    public init(
        callAnalyticsTranscriptResultStream: AsyncThrowingStream<TranscribeStreamingClientTypes.CallAnalyticsTranscriptResultStream, Swift.Error>? = nil,
        contentIdentificationType: TranscribeStreamingClientTypes.ContentIdentificationType? = nil,
        contentRedactionType: TranscribeStreamingClientTypes.ContentRedactionType? = nil,
        enablePartialResultsStabilization: Swift.Bool = false,
        languageCode: TranscribeStreamingClientTypes.CallAnalyticsLanguageCode? = nil,
        languageModelName: Swift.String? = nil,
        mediaEncoding: TranscribeStreamingClientTypes.MediaEncoding? = nil,
        mediaSampleRateHertz: Swift.Int? = nil,
        partialResultsStability: TranscribeStreamingClientTypes.PartialResultsStability? = nil,
        piiEntityTypes: Swift.String? = nil,
        requestId: Swift.String? = nil,
        sessionId: Swift.String? = nil,
        vocabularyFilterMethod: TranscribeStreamingClientTypes.VocabularyFilterMethod? = nil,
        vocabularyFilterName: Swift.String? = nil,
        vocabularyName: Swift.String? = nil
    )
    {
        self.callAnalyticsTranscriptResultStream = callAnalyticsTranscriptResultStream
        self.contentIdentificationType = contentIdentificationType
        self.contentRedactionType = contentRedactionType
        self.enablePartialResultsStabilization = enablePartialResultsStabilization
        self.languageCode = languageCode
        self.languageModelName = languageModelName
        self.mediaEncoding = mediaEncoding
        self.mediaSampleRateHertz = mediaSampleRateHertz
        self.partialResultsStability = partialResultsStability
        self.piiEntityTypes = piiEntityTypes
        self.requestId = requestId
        self.sessionId = sessionId
        self.vocabularyFilterMethod = vocabularyFilterMethod
        self.vocabularyFilterName = vocabularyFilterName
        self.vocabularyName = vocabularyName
    }
}

extension TranscribeStreamingClientTypes {

    public enum ModelType: Swift.Equatable, Swift.RawRepresentable, Swift.CaseIterable, Swift.Hashable {
        case conversation
        case dictation
        case sdkUnknown(Swift.String)

        public static var allCases: [ModelType] {
            return [
                .conversation,
                .dictation
            ]
        }

        public init?(rawValue: Swift.String) {
            let value = Self.allCases.first(where: { $0.rawValue == rawValue })
            self = value ?? Self.sdkUnknown(rawValue)
        }

        public var rawValue: Swift.String {
            switch self {
            case .conversation: return "CONVERSATION"
            case .dictation: return "DICTATION"
            case let .sdkUnknown(s): return s
            }
        }
    }
}

public struct StartMedicalStreamTranscriptionInput {
    /// An encoded stream of audio blobs. Audio streams are encoded as either HTTP/2 or WebSocket data frames. For more information, see [Transcribing streaming audio](https://docs.aws.amazon.com/transcribe/latest/dg/streaming.html).
    /// This member is required.
    public var audioStream: AsyncThrowingStream<TranscribeStreamingClientTypes.AudioStream, Swift.Error>?
    /// Labels all personal health information (PHI) identified in your transcript. Content identification is performed at the segment level; PHI is flagged upon complete transcription of an audio segment. For more information, see [Identifying personal health information (PHI) in a transcription](https://docs.aws.amazon.com/transcribe/latest/dg/phi-id.html).
    public var contentIdentificationType: TranscribeStreamingClientTypes.MedicalContentIdentificationType?
    /// Enables channel identification in multi-channel audio. Channel identification transcribes the audio on each channel independently, then appends the output for each channel into one transcript. If you have multi-channel audio and do not enable channel identification, your audio is transcribed in a continuous manner and your transcript is not separated by channel. For more information, see [Transcribing multi-channel audio](https://docs.aws.amazon.com/transcribe/latest/dg/channel-id.html).
    public var enableChannelIdentification: Swift.Bool?
    /// Specify the language code that represents the language spoken in your audio. Amazon Transcribe Medical only supports US English (en-US).
    /// This member is required.
    public var languageCode: TranscribeStreamingClientTypes.LanguageCode?
    /// Specify the encoding used for the input audio. Supported formats are:
    ///
    /// * FLAC
    ///
    /// * OPUS-encoded audio in an Ogg container
    ///
    /// * PCM (only signed 16-bit little-endian audio formats, which does not include WAV)
    ///
    ///
    /// For more information, see [Media formats](https://docs.aws.amazon.com/transcribe/latest/dg/how-input.html#how-input-audio).
    /// This member is required.
    public var mediaEncoding: TranscribeStreamingClientTypes.MediaEncoding?
    /// The sample rate of the input audio (in hertz). Amazon Transcribe Medical supports a range from 16,000 Hz to 48,000 Hz. Note that the sample rate you specify must match that of your audio.
    /// This member is required.
    public var mediaSampleRateHertz: Swift.Int?
    /// Specify the number of channels in your audio stream. Up to two channels are supported.
    public var numberOfChannels: Swift.Int?
    /// Specify a name for your transcription session. If you don't include this parameter in your request, Amazon Transcribe Medical generates an ID and returns it in the response. You can use a session ID to retry a streaming session.
    public var sessionId: Swift.String?
    /// Enables speaker partitioning (diarization) in your transcription output. Speaker partitioning labels the speech from individual speakers in your media file. For more information, see [Partitioning speakers (diarization)](https://docs.aws.amazon.com/transcribe/latest/dg/diarization.html).
    public var showSpeakerLabel: Swift.Bool?
    /// Specify the medical specialty contained in your audio.
    /// This member is required.
    public var specialty: TranscribeStreamingClientTypes.Specialty?
    /// Specify the type of input audio. For example, choose DICTATION for a provider dictating patient notes and CONVERSATION for a dialogue between a patient and a medical professional.
    /// This member is required.
    public var type: TranscribeStreamingClientTypes.ModelType?
    /// Specify the name of the custom vocabulary that you want to use when processing your transcription. Note that vocabulary names are case sensitive.
    public var vocabularyName: Swift.String?

    public init(
        audioStream: AsyncThrowingStream<TranscribeStreamingClientTypes.AudioStream, Swift.Error>? = nil,
        contentIdentificationType: TranscribeStreamingClientTypes.MedicalContentIdentificationType? = nil,
        enableChannelIdentification: Swift.Bool? = false,
        languageCode: TranscribeStreamingClientTypes.LanguageCode? = nil,
        mediaEncoding: TranscribeStreamingClientTypes.MediaEncoding? = nil,
        mediaSampleRateHertz: Swift.Int? = nil,
        numberOfChannels: Swift.Int? = nil,
        sessionId: Swift.String? = nil,
        showSpeakerLabel: Swift.Bool? = false,
        specialty: TranscribeStreamingClientTypes.Specialty? = nil,
        type: TranscribeStreamingClientTypes.ModelType? = nil,
        vocabularyName: Swift.String? = nil
    )
    {
        self.audioStream = audioStream
        self.contentIdentificationType = contentIdentificationType
        self.enableChannelIdentification = enableChannelIdentification
        self.languageCode = languageCode
        self.mediaEncoding = mediaEncoding
        self.mediaSampleRateHertz = mediaSampleRateHertz
        self.numberOfChannels = numberOfChannels
        self.sessionId = sessionId
        self.showSpeakerLabel = showSpeakerLabel
        self.specialty = specialty
        self.type = type
        self.vocabularyName = vocabularyName
    }
}

public struct StartMedicalStreamTranscriptionOutput {
    /// Shows whether content identification was enabled for your transcription.
    public var contentIdentificationType: TranscribeStreamingClientTypes.MedicalContentIdentificationType?
    /// Shows whether channel identification was enabled for your transcription.
    public var enableChannelIdentification: Swift.Bool
    /// Provides the language code that you specified in your request. This must be en-US.
    public var languageCode: TranscribeStreamingClientTypes.LanguageCode?
    /// Provides the media encoding you specified in your request.
    public var mediaEncoding: TranscribeStreamingClientTypes.MediaEncoding?
    /// Provides the sample rate that you specified in your request.
    public var mediaSampleRateHertz: Swift.Int?
    /// Provides the number of channels that you specified in your request.
    public var numberOfChannels: Swift.Int?
    /// Provides the identifier for your streaming request.
    public var requestId: Swift.String?
    /// Provides the identifier for your transcription session.
    public var sessionId: Swift.String?
    /// Shows whether speaker partitioning was enabled for your transcription.
    public var showSpeakerLabel: Swift.Bool
    /// Provides the medical specialty that you specified in your request.
    public var specialty: TranscribeStreamingClientTypes.Specialty?
    /// Provides detailed information about your streaming session.
    public var transcriptResultStream: AsyncThrowingStream<TranscribeStreamingClientTypes.MedicalTranscriptResultStream, Swift.Error>?
    /// Provides the type of audio you specified in your request.
    public var type: TranscribeStreamingClientTypes.ModelType?
    /// Provides the name of the custom vocabulary that you specified in your request.
    public var vocabularyName: Swift.String?

    public init(
        contentIdentificationType: TranscribeStreamingClientTypes.MedicalContentIdentificationType? = nil,
        enableChannelIdentification: Swift.Bool = false,
        languageCode: TranscribeStreamingClientTypes.LanguageCode? = nil,
        mediaEncoding: TranscribeStreamingClientTypes.MediaEncoding? = nil,
        mediaSampleRateHertz: Swift.Int? = nil,
        numberOfChannels: Swift.Int? = nil,
        requestId: Swift.String? = nil,
        sessionId: Swift.String? = nil,
        showSpeakerLabel: Swift.Bool = false,
        specialty: TranscribeStreamingClientTypes.Specialty? = nil,
        transcriptResultStream: AsyncThrowingStream<TranscribeStreamingClientTypes.MedicalTranscriptResultStream, Swift.Error>? = nil,
        type: TranscribeStreamingClientTypes.ModelType? = nil,
        vocabularyName: Swift.String? = nil
    )
    {
        self.contentIdentificationType = contentIdentificationType
        self.enableChannelIdentification = enableChannelIdentification
        self.languageCode = languageCode
        self.mediaEncoding = mediaEncoding
        self.mediaSampleRateHertz = mediaSampleRateHertz
        self.numberOfChannels = numberOfChannels
        self.requestId = requestId
        self.sessionId = sessionId
        self.showSpeakerLabel = showSpeakerLabel
        self.specialty = specialty
        self.transcriptResultStream = transcriptResultStream
        self.type = type
        self.vocabularyName = vocabularyName
    }
}

public struct StartStreamTranscriptionInput {
    /// An encoded stream of audio blobs. Audio streams are encoded as either HTTP/2 or WebSocket data frames. For more information, see [Transcribing streaming audio](https://docs.aws.amazon.com/transcribe/latest/dg/streaming.html).
    /// This member is required.
    public var audioStream: AsyncThrowingStream<TranscribeStreamingClientTypes.AudioStream, Swift.Error>?
    /// Labels all personally identifiable information (PII) identified in your transcript. Content identification is performed at the segment level; PII specified in PiiEntityTypes is flagged upon complete transcription of an audio segment. You canâ€™t set ContentIdentificationType and ContentRedactionType in the same request. If you set both, your request returns a BadRequestException. For more information, see [Redacting or identifying personally identifiable information](https://docs.aws.amazon.com/transcribe/latest/dg/pii-redaction.html).
    public var contentIdentificationType: TranscribeStreamingClientTypes.ContentIdentificationType?
    /// Redacts all personally identifiable information (PII) identified in your transcript. Content redaction is performed at the segment level; PII specified in PiiEntityTypes is redacted upon complete transcription of an audio segment. You canâ€™t set ContentRedactionType and ContentIdentificationType in the same request. If you set both, your request returns a BadRequestException. For more information, see [Redacting or identifying personally identifiable information](https://docs.aws.amazon.com/transcribe/latest/dg/pii-redaction.html).
    public var contentRedactionType: TranscribeStreamingClientTypes.ContentRedactionType?
    /// Enables channel identification in multi-channel audio. Channel identification transcribes the audio on each channel independently, then appends the output for each channel into one transcript. If you have multi-channel audio and do not enable channel identification, your audio is transcribed in a continuous manner and your transcript is not separated by channel. For more information, see [Transcribing multi-channel audio](https://docs.aws.amazon.com/transcribe/latest/dg/channel-id.html).
    public var enableChannelIdentification: Swift.Bool?
    /// Enables partial result stabilization for your transcription. Partial result stabilization can reduce latency in your output, but may impact accuracy. For more information, see [Partial-result stabilization](https://docs.aws.amazon.com/transcribe/latest/dg/streaming.html#streaming-partial-result-stabilization).
    public var enablePartialResultsStabilization: Swift.Bool?
    /// Enables automatic language identification for your transcription. If you include IdentifyLanguage, you can optionally include a list of language codes, using LanguageOptions, that you think may be present in your audio stream. Including language options can improve transcription accuracy. You can also include a preferred language using PreferredLanguage. Adding a preferred language can help Amazon Transcribe identify the language faster than if you omit this parameter. If you have multi-channel audio that contains different languages on each channel, and you've enabled channel identification, automatic language identification identifies the dominant language on each audio channel. Note that you must include either LanguageCode or IdentifyLanguage or IdentifyMultipleLanguages in your request. If you include more than one of these parameters, your transcription job fails. Streaming language identification can't be combined with custom language models or redaction.
    public var identifyLanguage: Swift.Bool?
    /// Enables automatic multi-language identification in your transcription job request. Use this parameter if your stream contains more than one language. If your stream contains only one language, use IdentifyLanguage instead. If you include IdentifyMultipleLanguages, you can optionally include a list of language codes, using LanguageOptions, that you think may be present in your stream. Including LanguageOptions restricts IdentifyMultipleLanguages to only the language options that you specify, which can improve transcription accuracy. If you want to apply a custom vocabulary or a custom vocabulary filter to your automatic multiple language identification request, include VocabularyNames or VocabularyFilterNames. Note that you must include one of LanguageCode, IdentifyLanguage, or IdentifyMultipleLanguages in your request. If you include more than one of these parameters, your transcription job fails.
    public var identifyMultipleLanguages: Swift.Bool?
    /// Specify the language code that represents the language spoken in your audio. If you're unsure of the language spoken in your audio, consider using IdentifyLanguage to enable automatic language identification. For a list of languages supported with Amazon Transcribe streaming, refer to the [Supported languages](https://docs.aws.amazon.com/transcribe/latest/dg/supported-languages.html) table.
    public var languageCode: TranscribeStreamingClientTypes.LanguageCode?
    /// Specify the name of the custom language model that you want to use when processing your transcription. Note that language model names are case sensitive. The language of the specified language model must match the language code you specify in your transcription request. If the languages don't match, the custom language model isn't applied. There are no errors or warnings associated with a language mismatch. For more information, see [Custom language models](https://docs.aws.amazon.com/transcribe/latest/dg/custom-language-models.html).
    public var languageModelName: Swift.String?
    /// Specify two or more language codes that represent the languages you think may be present in your media; including more than five is not recommended. If you're unsure what languages are present, do not include this parameter. Including language options can improve the accuracy of language identification. If you include LanguageOptions in your request, you must also include IdentifyLanguage. For a list of languages supported with Amazon Transcribe streaming, refer to the [Supported languages](https://docs.aws.amazon.com/transcribe/latest/dg/supported-languages.html) table. You can only include one language dialect per language per stream. For example, you cannot include en-US and en-AU in the same request.
    public var languageOptions: Swift.String?
    /// Specify the encoding of your input audio. Supported formats are:
    ///
    /// * FLAC
    ///
    /// * OPUS-encoded audio in an Ogg container
    ///
    /// * PCM (only signed 16-bit little-endian audio formats, which does not include WAV)
    ///
    ///
    /// For more information, see [Media formats](https://docs.aws.amazon.com/transcribe/latest/dg/how-input.html#how-input-audio).
    /// This member is required.
    public var mediaEncoding: TranscribeStreamingClientTypes.MediaEncoding?
    /// The sample rate of the input audio (in hertz). Low-quality audio, such as telephone audio, is typically around 8,000 Hz. High-quality audio typically ranges from 16,000 Hz to 48,000 Hz. Note that the sample rate you specify must match that of your audio.
    /// This member is required.
    public var mediaSampleRateHertz: Swift.Int?
    /// Specify the number of channels in your audio stream. Up to two channels are supported.
    public var numberOfChannels: Swift.Int?
    /// Specify the level of stability to use when you enable partial results stabilization (EnablePartialResultsStabilization). Low stability provides the highest accuracy. High stability transcribes faster, but with slightly lower accuracy. For more information, see [Partial-result stabilization](https://docs.aws.amazon.com/transcribe/latest/dg/streaming.html#streaming-partial-result-stabilization).
    public var partialResultsStability: TranscribeStreamingClientTypes.PartialResultsStability?
    /// Specify which types of personally identifiable information (PII) you want to redact in your transcript. You can include as many types as you'd like, or you can select ALL. To include PiiEntityTypes in your request, you must also include either ContentIdentificationType or ContentRedactionType. Values must be comma-separated and can include: BANK_ACCOUNT_NUMBER, BANK_ROUTING, CREDIT_DEBIT_NUMBER, CREDIT_DEBIT_CVV, CREDIT_DEBIT_EXPIRY, PIN, EMAIL, ADDRESS, NAME, PHONE, SSN, or ALL.
    public var piiEntityTypes: Swift.String?
    /// Specify a preferred language from the subset of languages codes you specified in LanguageOptions. You can only use this parameter if you've included IdentifyLanguage and LanguageOptions in your request.
    public var preferredLanguage: TranscribeStreamingClientTypes.LanguageCode?
    /// Specify a name for your transcription session. If you don't include this parameter in your request, Amazon Transcribe generates an ID and returns it in the response. You can use a session ID to retry a streaming session.
    public var sessionId: Swift.String?
    /// Enables speaker partitioning (diarization) in your transcription output. Speaker partitioning labels the speech from individual speakers in your media file. For more information, see [Partitioning speakers (diarization)](https://docs.aws.amazon.com/transcribe/latest/dg/diarization.html).
    public var showSpeakerLabel: Swift.Bool?
    /// Specify how you want your vocabulary filter applied to your transcript. To replace words with ***, choose mask. To delete words, choose remove. To flag words without changing them, choose tag.
    public var vocabularyFilterMethod: TranscribeStreamingClientTypes.VocabularyFilterMethod?
    /// Specify the name of the custom vocabulary filter that you want to use when processing your transcription. Note that vocabulary filter names are case sensitive. If the language of the specified custom vocabulary filter doesn't match the language identified in your media, the vocabulary filter is not applied to your transcription. This parameter is not intended for use with the IdentifyLanguage parameter. If you're including IdentifyLanguage in your request and want to use one or more vocabulary filters with your transcription, use the VocabularyFilterNames parameter instead. For more information, see [Using vocabulary filtering with unwanted words](https://docs.aws.amazon.com/transcribe/latest/dg/vocabulary-filtering.html).
    public var vocabularyFilterName: Swift.String?
    /// Specify the names of the custom vocabulary filters that you want to use when processing your transcription. Note that vocabulary filter names are case sensitive. If none of the languages of the specified custom vocabulary filters match the language identified in your media, your job fails. This parameter is only intended for use with the IdentifyLanguage parameter. If you're not including IdentifyLanguage in your request and want to use a custom vocabulary filter with your transcription, use the VocabularyFilterName parameter instead. For more information, see [Using vocabulary filtering with unwanted words](https://docs.aws.amazon.com/transcribe/latest/dg/vocabulary-filtering.html).
    public var vocabularyFilterNames: Swift.String?
    /// Specify the name of the custom vocabulary that you want to use when processing your transcription. Note that vocabulary names are case sensitive. If the language of the specified custom vocabulary doesn't match the language identified in your media, the custom vocabulary is not applied to your transcription. This parameter is not intended for use with the IdentifyLanguage parameter. If you're including IdentifyLanguage in your request and want to use one or more custom vocabularies with your transcription, use the VocabularyNames parameter instead. For more information, see [Custom vocabularies](https://docs.aws.amazon.com/transcribe/latest/dg/custom-vocabulary.html).
    public var vocabularyName: Swift.String?
    /// Specify the names of the custom vocabularies that you want to use when processing your transcription. Note that vocabulary names are case sensitive. If none of the languages of the specified custom vocabularies match the language identified in your media, your job fails. This parameter is only intended for use with the IdentifyLanguage parameter. If you're not including IdentifyLanguage in your request and want to use a custom vocabulary with your transcription, use the VocabularyName parameter instead. For more information, see [Custom vocabularies](https://docs.aws.amazon.com/transcribe/latest/dg/custom-vocabulary.html).
    public var vocabularyNames: Swift.String?

    public init(
        audioStream: AsyncThrowingStream<TranscribeStreamingClientTypes.AudioStream, Swift.Error>? = nil,
        contentIdentificationType: TranscribeStreamingClientTypes.ContentIdentificationType? = nil,
        contentRedactionType: TranscribeStreamingClientTypes.ContentRedactionType? = nil,
        enableChannelIdentification: Swift.Bool? = false,
        enablePartialResultsStabilization: Swift.Bool? = false,
        identifyLanguage: Swift.Bool? = false,
        identifyMultipleLanguages: Swift.Bool? = false,
        languageCode: TranscribeStreamingClientTypes.LanguageCode? = nil,
        languageModelName: Swift.String? = nil,
        languageOptions: Swift.String? = nil,
        mediaEncoding: TranscribeStreamingClientTypes.MediaEncoding? = nil,
        mediaSampleRateHertz: Swift.Int? = nil,
        numberOfChannels: Swift.Int? = nil,
        partialResultsStability: TranscribeStreamingClientTypes.PartialResultsStability? = nil,
        piiEntityTypes: Swift.String? = nil,
        preferredLanguage: TranscribeStreamingClientTypes.LanguageCode? = nil,
        sessionId: Swift.String? = nil,
        showSpeakerLabel: Swift.Bool? = false,
        vocabularyFilterMethod: TranscribeStreamingClientTypes.VocabularyFilterMethod? = nil,
        vocabularyFilterName: Swift.String? = nil,
        vocabularyFilterNames: Swift.String? = nil,
        vocabularyName: Swift.String? = nil,
        vocabularyNames: Swift.String? = nil
    )
    {
        self.audioStream = audioStream
        self.contentIdentificationType = contentIdentificationType
        self.contentRedactionType = contentRedactionType
        self.enableChannelIdentification = enableChannelIdentification
        self.enablePartialResultsStabilization = enablePartialResultsStabilization
        self.identifyLanguage = identifyLanguage
        self.identifyMultipleLanguages = identifyMultipleLanguages
        self.languageCode = languageCode
        self.languageModelName = languageModelName
        self.languageOptions = languageOptions
        self.mediaEncoding = mediaEncoding
        self.mediaSampleRateHertz = mediaSampleRateHertz
        self.numberOfChannels = numberOfChannels
        self.partialResultsStability = partialResultsStability
        self.piiEntityTypes = piiEntityTypes
        self.preferredLanguage = preferredLanguage
        self.sessionId = sessionId
        self.showSpeakerLabel = showSpeakerLabel
        self.vocabularyFilterMethod = vocabularyFilterMethod
        self.vocabularyFilterName = vocabularyFilterName
        self.vocabularyFilterNames = vocabularyFilterNames
        self.vocabularyName = vocabularyName
        self.vocabularyNames = vocabularyNames
    }
}

extension TranscribeStreamingClientTypes {
    /// The Transcript associated with a . Transcript contains Results, which contains a set of transcription results from one or more audio segments, along with additional information per your request parameters.
    public struct Transcript {
        /// Contains a set of transcription results from one or more audio segments, along with additional information per your request parameters. This can include information relating to alternative transcriptions, channel identification, partial result stabilization, language identification, and other transcription-related data.
        public var results: [TranscribeStreamingClientTypes.Result]?

        public init(
            results: [TranscribeStreamingClientTypes.Result]? = nil
        )
        {
            self.results = results
        }
    }

}

extension TranscribeStreamingClientTypes {
    /// The TranscriptEvent associated with a TranscriptResultStream. Contains a set of transcription results from one or more audio segments, along with additional information per your request parameters.
    public struct TranscriptEvent {
        /// Contains Results, which contains a set of transcription results from one or more audio segments, along with additional information per your request parameters. This can include information relating to alternative transcriptions, channel identification, partial result stabilization, language identification, and other transcription-related data.
        public var transcript: TranscribeStreamingClientTypes.Transcript?

        public init(
            transcript: TranscribeStreamingClientTypes.Transcript? = nil
        )
        {
            self.transcript = transcript
        }
    }

}

extension TranscribeStreamingClientTypes {
    /// Contains detailed information about your streaming session.
    public enum TranscriptResultStream {
        /// Contains Transcript, which contains Results. The  object contains a set of transcription results from one or more audio segments, along with additional information per your request parameters.
        case transcriptevent(TranscribeStreamingClientTypes.TranscriptEvent)
        case sdkUnknown(Swift.String)
    }

}

public struct StartStreamTranscriptionOutput {
    /// Shows whether content identification was enabled for your transcription.
    public var contentIdentificationType: TranscribeStreamingClientTypes.ContentIdentificationType?
    /// Shows whether content redaction was enabled for your transcription.
    public var contentRedactionType: TranscribeStreamingClientTypes.ContentRedactionType?
    /// Shows whether channel identification was enabled for your transcription.
    public var enableChannelIdentification: Swift.Bool
    /// Shows whether partial results stabilization was enabled for your transcription.
    public var enablePartialResultsStabilization: Swift.Bool
    /// Shows whether automatic language identification was enabled for your transcription.
    public var identifyLanguage: Swift.Bool
    /// Shows whether automatic multi-language identification was enabled for your transcription.
    public var identifyMultipleLanguages: Swift.Bool
    /// Provides the language code that you specified in your request.
    public var languageCode: TranscribeStreamingClientTypes.LanguageCode?
    /// Provides the name of the custom language model that you specified in your request.
    public var languageModelName: Swift.String?
    /// Provides the language codes that you specified in your request.
    public var languageOptions: Swift.String?
    /// Provides the media encoding you specified in your request.
    public var mediaEncoding: TranscribeStreamingClientTypes.MediaEncoding?
    /// Provides the sample rate that you specified in your request.
    public var mediaSampleRateHertz: Swift.Int?
    /// Provides the number of channels that you specified in your request.
    public var numberOfChannels: Swift.Int?
    /// Provides the stabilization level used for your transcription.
    public var partialResultsStability: TranscribeStreamingClientTypes.PartialResultsStability?
    /// Lists the PII entity types you specified in your request.
    public var piiEntityTypes: Swift.String?
    /// Provides the preferred language that you specified in your request.
    public var preferredLanguage: TranscribeStreamingClientTypes.LanguageCode?
    /// Provides the identifier for your streaming request.
    public var requestId: Swift.String?
    /// Provides the identifier for your transcription session.
    public var sessionId: Swift.String?
    /// Shows whether speaker partitioning was enabled for your transcription.
    public var showSpeakerLabel: Swift.Bool
    /// Provides detailed information about your streaming session.
    public var transcriptResultStream: AsyncThrowingStream<TranscribeStreamingClientTypes.TranscriptResultStream, Swift.Error>?
    /// Provides the vocabulary filtering method used in your transcription.
    public var vocabularyFilterMethod: TranscribeStreamingClientTypes.VocabularyFilterMethod?
    /// Provides the name of the custom vocabulary filter that you specified in your request.
    public var vocabularyFilterName: Swift.String?
    /// Provides the names of the custom vocabulary filters that you specified in your request.
    public var vocabularyFilterNames: Swift.String?
    /// Provides the name of the custom vocabulary that you specified in your request.
    public var vocabularyName: Swift.String?
    /// Provides the names of the custom vocabularies that you specified in your request.
    public var vocabularyNames: Swift.String?

    public init(
        contentIdentificationType: TranscribeStreamingClientTypes.ContentIdentificationType? = nil,
        contentRedactionType: TranscribeStreamingClientTypes.ContentRedactionType? = nil,
        enableChannelIdentification: Swift.Bool = false,
        enablePartialResultsStabilization: Swift.Bool = false,
        identifyLanguage: Swift.Bool = false,
        identifyMultipleLanguages: Swift.Bool = false,
        languageCode: TranscribeStreamingClientTypes.LanguageCode? = nil,
        languageModelName: Swift.String? = nil,
        languageOptions: Swift.String? = nil,
        mediaEncoding: TranscribeStreamingClientTypes.MediaEncoding? = nil,
        mediaSampleRateHertz: Swift.Int? = nil,
        numberOfChannels: Swift.Int? = nil,
        partialResultsStability: TranscribeStreamingClientTypes.PartialResultsStability? = nil,
        piiEntityTypes: Swift.String? = nil,
        preferredLanguage: TranscribeStreamingClientTypes.LanguageCode? = nil,
        requestId: Swift.String? = nil,
        sessionId: Swift.String? = nil,
        showSpeakerLabel: Swift.Bool = false,
        transcriptResultStream: AsyncThrowingStream<TranscribeStreamingClientTypes.TranscriptResultStream, Swift.Error>? = nil,
        vocabularyFilterMethod: TranscribeStreamingClientTypes.VocabularyFilterMethod? = nil,
        vocabularyFilterName: Swift.String? = nil,
        vocabularyFilterNames: Swift.String? = nil,
        vocabularyName: Swift.String? = nil,
        vocabularyNames: Swift.String? = nil
    )
    {
        self.contentIdentificationType = contentIdentificationType
        self.contentRedactionType = contentRedactionType
        self.enableChannelIdentification = enableChannelIdentification
        self.enablePartialResultsStabilization = enablePartialResultsStabilization
        self.identifyLanguage = identifyLanguage
        self.identifyMultipleLanguages = identifyMultipleLanguages
        self.languageCode = languageCode
        self.languageModelName = languageModelName
        self.languageOptions = languageOptions
        self.mediaEncoding = mediaEncoding
        self.mediaSampleRateHertz = mediaSampleRateHertz
        self.numberOfChannels = numberOfChannels
        self.partialResultsStability = partialResultsStability
        self.piiEntityTypes = piiEntityTypes
        self.preferredLanguage = preferredLanguage
        self.requestId = requestId
        self.sessionId = sessionId
        self.showSpeakerLabel = showSpeakerLabel
        self.transcriptResultStream = transcriptResultStream
        self.vocabularyFilterMethod = vocabularyFilterMethod
        self.vocabularyFilterName = vocabularyFilterName
        self.vocabularyFilterNames = vocabularyFilterNames
        self.vocabularyName = vocabularyName
        self.vocabularyNames = vocabularyNames
    }
}

extension StartCallAnalyticsStreamTranscriptionInput {

    static func urlPathProvider(_ value: StartCallAnalyticsStreamTranscriptionInput) -> Swift.String? {
        return "/call-analytics-stream-transcription"
    }
}

extension StartCallAnalyticsStreamTranscriptionInput {

    static func headerProvider(_ value: StartCallAnalyticsStreamTranscriptionInput) -> SmithyHTTPAPI.Headers {
        var items = SmithyHTTPAPI.Headers()
        if let contentIdentificationType = value.contentIdentificationType {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-content-identification-type", value: Swift.String(contentIdentificationType.rawValue)))
        }
        if let contentRedactionType = value.contentRedactionType {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-content-redaction-type", value: Swift.String(contentRedactionType.rawValue)))
        }
        if let enablePartialResultsStabilization = value.enablePartialResultsStabilization {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-enable-partial-results-stabilization", value: Swift.String(enablePartialResultsStabilization)))
        }
        if let languageCode = value.languageCode {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-language-code", value: Swift.String(languageCode.rawValue)))
        }
        if let languageModelName = value.languageModelName {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-language-model-name", value: Swift.String(languageModelName)))
        }
        if let mediaEncoding = value.mediaEncoding {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-media-encoding", value: Swift.String(mediaEncoding.rawValue)))
        }
        if let mediaSampleRateHertz = value.mediaSampleRateHertz {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-sample-rate", value: Swift.String(mediaSampleRateHertz)))
        }
        if let partialResultsStability = value.partialResultsStability {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-partial-results-stability", value: Swift.String(partialResultsStability.rawValue)))
        }
        if let piiEntityTypes = value.piiEntityTypes {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-pii-entity-types", value: Swift.String(piiEntityTypes)))
        }
        if let sessionId = value.sessionId {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-session-id", value: Swift.String(sessionId)))
        }
        if let vocabularyFilterMethod = value.vocabularyFilterMethod {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-vocabulary-filter-method", value: Swift.String(vocabularyFilterMethod.rawValue)))
        }
        if let vocabularyFilterName = value.vocabularyFilterName {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-vocabulary-filter-name", value: Swift.String(vocabularyFilterName)))
        }
        if let vocabularyName = value.vocabularyName {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-vocabulary-name", value: Swift.String(vocabularyName)))
        }
        return items
    }
}

extension StartMedicalStreamTranscriptionInput {

    static func urlPathProvider(_ value: StartMedicalStreamTranscriptionInput) -> Swift.String? {
        return "/medical-stream-transcription"
    }
}

extension StartMedicalStreamTranscriptionInput {

    static func headerProvider(_ value: StartMedicalStreamTranscriptionInput) -> SmithyHTTPAPI.Headers {
        var items = SmithyHTTPAPI.Headers()
        if let contentIdentificationType = value.contentIdentificationType {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-content-identification-type", value: Swift.String(contentIdentificationType.rawValue)))
        }
        if let enableChannelIdentification = value.enableChannelIdentification {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-enable-channel-identification", value: Swift.String(enableChannelIdentification)))
        }
        if let languageCode = value.languageCode {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-language-code", value: Swift.String(languageCode.rawValue)))
        }
        if let mediaEncoding = value.mediaEncoding {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-media-encoding", value: Swift.String(mediaEncoding.rawValue)))
        }
        if let mediaSampleRateHertz = value.mediaSampleRateHertz {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-sample-rate", value: Swift.String(mediaSampleRateHertz)))
        }
        if let numberOfChannels = value.numberOfChannels {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-number-of-channels", value: Swift.String(numberOfChannels)))
        }
        if let sessionId = value.sessionId {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-session-id", value: Swift.String(sessionId)))
        }
        if let showSpeakerLabel = value.showSpeakerLabel {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-show-speaker-label", value: Swift.String(showSpeakerLabel)))
        }
        if let specialty = value.specialty {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-specialty", value: Swift.String(specialty.rawValue)))
        }
        if let type = value.type {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-type", value: Swift.String(type.rawValue)))
        }
        if let vocabularyName = value.vocabularyName {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-vocabulary-name", value: Swift.String(vocabularyName)))
        }
        return items
    }
}

extension StartStreamTranscriptionInput {

    static func urlPathProvider(_ value: StartStreamTranscriptionInput) -> Swift.String? {
        return "/stream-transcription"
    }
}

extension StartStreamTranscriptionInput {

    static func headerProvider(_ value: StartStreamTranscriptionInput) -> SmithyHTTPAPI.Headers {
        var items = SmithyHTTPAPI.Headers()
        if let contentIdentificationType = value.contentIdentificationType {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-content-identification-type", value: Swift.String(contentIdentificationType.rawValue)))
        }
        if let contentRedactionType = value.contentRedactionType {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-content-redaction-type", value: Swift.String(contentRedactionType.rawValue)))
        }
        if let enableChannelIdentification = value.enableChannelIdentification {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-enable-channel-identification", value: Swift.String(enableChannelIdentification)))
        }
        if let enablePartialResultsStabilization = value.enablePartialResultsStabilization {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-enable-partial-results-stabilization", value: Swift.String(enablePartialResultsStabilization)))
        }
        if let identifyLanguage = value.identifyLanguage {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-identify-language", value: Swift.String(identifyLanguage)))
        }
        if let identifyMultipleLanguages = value.identifyMultipleLanguages {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-identify-multiple-languages", value: Swift.String(identifyMultipleLanguages)))
        }
        if let languageCode = value.languageCode {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-language-code", value: Swift.String(languageCode.rawValue)))
        }
        if let languageModelName = value.languageModelName {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-language-model-name", value: Swift.String(languageModelName)))
        }
        if let languageOptions = value.languageOptions {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-language-options", value: Swift.String(languageOptions)))
        }
        if let mediaEncoding = value.mediaEncoding {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-media-encoding", value: Swift.String(mediaEncoding.rawValue)))
        }
        if let mediaSampleRateHertz = value.mediaSampleRateHertz {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-sample-rate", value: Swift.String(mediaSampleRateHertz)))
        }
        if let numberOfChannels = value.numberOfChannels {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-number-of-channels", value: Swift.String(numberOfChannels)))
        }
        if let partialResultsStability = value.partialResultsStability {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-partial-results-stability", value: Swift.String(partialResultsStability.rawValue)))
        }
        if let piiEntityTypes = value.piiEntityTypes {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-pii-entity-types", value: Swift.String(piiEntityTypes)))
        }
        if let preferredLanguage = value.preferredLanguage {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-preferred-language", value: Swift.String(preferredLanguage.rawValue)))
        }
        if let sessionId = value.sessionId {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-session-id", value: Swift.String(sessionId)))
        }
        if let showSpeakerLabel = value.showSpeakerLabel {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-show-speaker-label", value: Swift.String(showSpeakerLabel)))
        }
        if let vocabularyFilterMethod = value.vocabularyFilterMethod {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-vocabulary-filter-method", value: Swift.String(vocabularyFilterMethod.rawValue)))
        }
        if let vocabularyFilterName = value.vocabularyFilterName {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-vocabulary-filter-name", value: Swift.String(vocabularyFilterName)))
        }
        if let vocabularyFilterNames = value.vocabularyFilterNames {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-vocabulary-filter-names", value: Swift.String(vocabularyFilterNames)))
        }
        if let vocabularyName = value.vocabularyName {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-vocabulary-name", value: Swift.String(vocabularyName)))
        }
        if let vocabularyNames = value.vocabularyNames {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-vocabulary-names", value: Swift.String(vocabularyNames)))
        }
        return items
    }
}

extension StartCallAnalyticsStreamTranscriptionOutput {

    static func httpOutput(from httpResponse: SmithyHTTPAPI.HTTPResponse) async throws -> StartCallAnalyticsStreamTranscriptionOutput {
        var value = StartCallAnalyticsStreamTranscriptionOutput()
        if let contentIdentificationTypeHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-content-identification-type") {
            value.contentIdentificationType = TranscribeStreamingClientTypes.ContentIdentificationType(rawValue: contentIdentificationTypeHeaderValue)
        }
        if let contentRedactionTypeHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-content-redaction-type") {
            value.contentRedactionType = TranscribeStreamingClientTypes.ContentRedactionType(rawValue: contentRedactionTypeHeaderValue)
        }
        if let enablePartialResultsStabilizationHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-enable-partial-results-stabilization") {
            value.enablePartialResultsStabilization = Swift.Bool(enablePartialResultsStabilizationHeaderValue) ?? false
        }
        if let languageCodeHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-language-code") {
            value.languageCode = TranscribeStreamingClientTypes.CallAnalyticsLanguageCode(rawValue: languageCodeHeaderValue)
        }
        if let languageModelNameHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-language-model-name") {
            value.languageModelName = languageModelNameHeaderValue
        }
        if let mediaEncodingHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-media-encoding") {
            value.mediaEncoding = TranscribeStreamingClientTypes.MediaEncoding(rawValue: mediaEncodingHeaderValue)
        }
        if let mediaSampleRateHertzHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-sample-rate") {
            value.mediaSampleRateHertz = Swift.Int(mediaSampleRateHertzHeaderValue) ?? 0
        }
        if let partialResultsStabilityHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-partial-results-stability") {
            value.partialResultsStability = TranscribeStreamingClientTypes.PartialResultsStability(rawValue: partialResultsStabilityHeaderValue)
        }
        if let piiEntityTypesHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-pii-entity-types") {
            value.piiEntityTypes = piiEntityTypesHeaderValue
        }
        if let requestIdHeaderValue = httpResponse.headers.value(for: "x-amzn-request-id") {
            value.requestId = requestIdHeaderValue
        }
        if let sessionIdHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-session-id") {
            value.sessionId = sessionIdHeaderValue
        }
        if let vocabularyFilterMethodHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-vocabulary-filter-method") {
            value.vocabularyFilterMethod = TranscribeStreamingClientTypes.VocabularyFilterMethod(rawValue: vocabularyFilterMethodHeaderValue)
        }
        if let vocabularyFilterNameHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-vocabulary-filter-name") {
            value.vocabularyFilterName = vocabularyFilterNameHeaderValue
        }
        if let vocabularyNameHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-vocabulary-name") {
            value.vocabularyName = vocabularyNameHeaderValue
        }
        if case .stream(let stream) = httpResponse.body {
            let messageDecoder = SmithyEventStreams.DefaultMessageDecoder()
            let decoderStream = SmithyEventStreams.DefaultMessageDecoderStream(stream: stream, messageDecoder: messageDecoder, unmarshalClosure: TranscribeStreamingClientTypes.CallAnalyticsTranscriptResultStream.unmarshal)
            value.callAnalyticsTranscriptResultStream = decoderStream.toAsyncStream()
        }
        return value
    }
}

extension StartMedicalStreamTranscriptionOutput {

    static func httpOutput(from httpResponse: SmithyHTTPAPI.HTTPResponse) async throws -> StartMedicalStreamTranscriptionOutput {
        var value = StartMedicalStreamTranscriptionOutput()
        if let contentIdentificationTypeHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-content-identification-type") {
            value.contentIdentificationType = TranscribeStreamingClientTypes.MedicalContentIdentificationType(rawValue: contentIdentificationTypeHeaderValue)
        }
        if let enableChannelIdentificationHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-enable-channel-identification") {
            value.enableChannelIdentification = Swift.Bool(enableChannelIdentificationHeaderValue) ?? false
        }
        if let languageCodeHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-language-code") {
            value.languageCode = TranscribeStreamingClientTypes.LanguageCode(rawValue: languageCodeHeaderValue)
        }
        if let mediaEncodingHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-media-encoding") {
            value.mediaEncoding = TranscribeStreamingClientTypes.MediaEncoding(rawValue: mediaEncodingHeaderValue)
        }
        if let mediaSampleRateHertzHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-sample-rate") {
            value.mediaSampleRateHertz = Swift.Int(mediaSampleRateHertzHeaderValue) ?? 0
        }
        if let numberOfChannelsHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-number-of-channels") {
            value.numberOfChannels = Swift.Int(numberOfChannelsHeaderValue) ?? 0
        }
        if let requestIdHeaderValue = httpResponse.headers.value(for: "x-amzn-request-id") {
            value.requestId = requestIdHeaderValue
        }
        if let sessionIdHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-session-id") {
            value.sessionId = sessionIdHeaderValue
        }
        if let showSpeakerLabelHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-show-speaker-label") {
            value.showSpeakerLabel = Swift.Bool(showSpeakerLabelHeaderValue) ?? false
        }
        if let specialtyHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-specialty") {
            value.specialty = TranscribeStreamingClientTypes.Specialty(rawValue: specialtyHeaderValue)
        }
        if let typeHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-type") {
            value.type = TranscribeStreamingClientTypes.ModelType(rawValue: typeHeaderValue)
        }
        if let vocabularyNameHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-vocabulary-name") {
            value.vocabularyName = vocabularyNameHeaderValue
        }
        if case .stream(let stream) = httpResponse.body {
            let messageDecoder = SmithyEventStreams.DefaultMessageDecoder()
            let decoderStream = SmithyEventStreams.DefaultMessageDecoderStream(stream: stream, messageDecoder: messageDecoder, unmarshalClosure: TranscribeStreamingClientTypes.MedicalTranscriptResultStream.unmarshal)
            value.transcriptResultStream = decoderStream.toAsyncStream()
        }
        return value
    }
}

extension StartStreamTranscriptionOutput {

    static func httpOutput(from httpResponse: SmithyHTTPAPI.HTTPResponse) async throws -> StartStreamTranscriptionOutput {
        var value = StartStreamTranscriptionOutput()
        if let contentIdentificationTypeHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-content-identification-type") {
            value.contentIdentificationType = TranscribeStreamingClientTypes.ContentIdentificationType(rawValue: contentIdentificationTypeHeaderValue)
        }
        if let contentRedactionTypeHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-content-redaction-type") {
            value.contentRedactionType = TranscribeStreamingClientTypes.ContentRedactionType(rawValue: contentRedactionTypeHeaderValue)
        }
        if let enableChannelIdentificationHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-enable-channel-identification") {
            value.enableChannelIdentification = Swift.Bool(enableChannelIdentificationHeaderValue) ?? false
        }
        if let enablePartialResultsStabilizationHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-enable-partial-results-stabilization") {
            value.enablePartialResultsStabilization = Swift.Bool(enablePartialResultsStabilizationHeaderValue) ?? false
        }
        if let identifyLanguageHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-identify-language") {
            value.identifyLanguage = Swift.Bool(identifyLanguageHeaderValue) ?? false
        }
        if let identifyMultipleLanguagesHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-identify-multiple-languages") {
            value.identifyMultipleLanguages = Swift.Bool(identifyMultipleLanguagesHeaderValue) ?? false
        }
        if let languageCodeHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-language-code") {
            value.languageCode = TranscribeStreamingClientTypes.LanguageCode(rawValue: languageCodeHeaderValue)
        }
        if let languageModelNameHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-language-model-name") {
            value.languageModelName = languageModelNameHeaderValue
        }
        if let languageOptionsHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-language-options") {
            value.languageOptions = languageOptionsHeaderValue
        }
        if let mediaEncodingHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-media-encoding") {
            value.mediaEncoding = TranscribeStreamingClientTypes.MediaEncoding(rawValue: mediaEncodingHeaderValue)
        }
        if let mediaSampleRateHertzHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-sample-rate") {
            value.mediaSampleRateHertz = Swift.Int(mediaSampleRateHertzHeaderValue) ?? 0
        }
        if let numberOfChannelsHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-number-of-channels") {
            value.numberOfChannels = Swift.Int(numberOfChannelsHeaderValue) ?? 0
        }
        if let partialResultsStabilityHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-partial-results-stability") {
            value.partialResultsStability = TranscribeStreamingClientTypes.PartialResultsStability(rawValue: partialResultsStabilityHeaderValue)
        }
        if let piiEntityTypesHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-pii-entity-types") {
            value.piiEntityTypes = piiEntityTypesHeaderValue
        }
        if let preferredLanguageHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-preferred-language") {
            value.preferredLanguage = TranscribeStreamingClientTypes.LanguageCode(rawValue: preferredLanguageHeaderValue)
        }
        if let requestIdHeaderValue = httpResponse.headers.value(for: "x-amzn-request-id") {
            value.requestId = requestIdHeaderValue
        }
        if let sessionIdHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-session-id") {
            value.sessionId = sessionIdHeaderValue
        }
        if let showSpeakerLabelHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-show-speaker-label") {
            value.showSpeakerLabel = Swift.Bool(showSpeakerLabelHeaderValue) ?? false
        }
        if let vocabularyFilterMethodHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-vocabulary-filter-method") {
            value.vocabularyFilterMethod = TranscribeStreamingClientTypes.VocabularyFilterMethod(rawValue: vocabularyFilterMethodHeaderValue)
        }
        if let vocabularyFilterNameHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-vocabulary-filter-name") {
            value.vocabularyFilterName = vocabularyFilterNameHeaderValue
        }
        if let vocabularyFilterNamesHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-vocabulary-filter-names") {
            value.vocabularyFilterNames = vocabularyFilterNamesHeaderValue
        }
        if let vocabularyNameHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-vocabulary-name") {
            value.vocabularyName = vocabularyNameHeaderValue
        }
        if let vocabularyNamesHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-vocabulary-names") {
            value.vocabularyNames = vocabularyNamesHeaderValue
        }
        if case .stream(let stream) = httpResponse.body {
            let messageDecoder = SmithyEventStreams.DefaultMessageDecoder()
            let decoderStream = SmithyEventStreams.DefaultMessageDecoderStream(stream: stream, messageDecoder: messageDecoder, unmarshalClosure: TranscribeStreamingClientTypes.TranscriptResultStream.unmarshal)
            value.transcriptResultStream = decoderStream.toAsyncStream()
        }
        return value
    }
}

enum StartCallAnalyticsStreamTranscriptionOutputError {

    static func httpError(from httpResponse: SmithyHTTPAPI.HTTPResponse) async throws -> Swift.Error {
        let data = try await httpResponse.data()
        let responseReader = try SmithyJSON.Reader.from(data: data)
        let baseError = try AWSClientRuntime.RestJSONError(httpResponse: httpResponse, responseReader: responseReader, noErrorWrapping: false)
        if let error = baseError.customError() { return error }
        switch baseError.code {
            case "BadRequestException": return try BadRequestException.makeError(baseError: baseError)
            case "ConflictException": return try ConflictException.makeError(baseError: baseError)
            case "InternalFailureException": return try InternalFailureException.makeError(baseError: baseError)
            case "LimitExceededException": return try LimitExceededException.makeError(baseError: baseError)
            case "ServiceUnavailableException": return try ServiceUnavailableException.makeError(baseError: baseError)
            default: return try AWSClientRuntime.UnknownAWSHTTPServiceError.makeError(baseError: baseError)
        }
    }
}

enum StartMedicalStreamTranscriptionOutputError {

    static func httpError(from httpResponse: SmithyHTTPAPI.HTTPResponse) async throws -> Swift.Error {
        let data = try await httpResponse.data()
        let responseReader = try SmithyJSON.Reader.from(data: data)
        let baseError = try AWSClientRuntime.RestJSONError(httpResponse: httpResponse, responseReader: responseReader, noErrorWrapping: false)
        if let error = baseError.customError() { return error }
        switch baseError.code {
            case "BadRequestException": return try BadRequestException.makeError(baseError: baseError)
            case "ConflictException": return try ConflictException.makeError(baseError: baseError)
            case "InternalFailureException": return try InternalFailureException.makeError(baseError: baseError)
            case "LimitExceededException": return try LimitExceededException.makeError(baseError: baseError)
            case "ServiceUnavailableException": return try ServiceUnavailableException.makeError(baseError: baseError)
            default: return try AWSClientRuntime.UnknownAWSHTTPServiceError.makeError(baseError: baseError)
        }
    }
}

enum StartStreamTranscriptionOutputError {

    static func httpError(from httpResponse: SmithyHTTPAPI.HTTPResponse) async throws -> Swift.Error {
        let data = try await httpResponse.data()
        let responseReader = try SmithyJSON.Reader.from(data: data)
        let baseError = try AWSClientRuntime.RestJSONError(httpResponse: httpResponse, responseReader: responseReader, noErrorWrapping: false)
        if let error = baseError.customError() { return error }
        switch baseError.code {
            case "BadRequestException": return try BadRequestException.makeError(baseError: baseError)
            case "ConflictException": return try ConflictException.makeError(baseError: baseError)
            case "InternalFailureException": return try InternalFailureException.makeError(baseError: baseError)
            case "LimitExceededException": return try LimitExceededException.makeError(baseError: baseError)
            case "ServiceUnavailableException": return try ServiceUnavailableException.makeError(baseError: baseError)
            default: return try AWSClientRuntime.UnknownAWSHTTPServiceError.makeError(baseError: baseError)
        }
    }
}

extension ServiceUnavailableException {

    static func makeError(baseError: AWSClientRuntime.RestJSONError) throws -> ServiceUnavailableException {
        let reader = baseError.errorBodyReader
        var value = ServiceUnavailableException()
        value.properties.message = try reader["Message"].readIfPresent()
        value.httpResponse = baseError.httpResponse
        value.requestID = baseError.requestID
        value.message = baseError.message
        return value
    }
}

extension BadRequestException {

    static func makeError(baseError: AWSClientRuntime.RestJSONError) throws -> BadRequestException {
        let reader = baseError.errorBodyReader
        var value = BadRequestException()
        value.properties.message = try reader["Message"].readIfPresent()
        value.httpResponse = baseError.httpResponse
        value.requestID = baseError.requestID
        value.message = baseError.message
        return value
    }
}

extension InternalFailureException {

    static func makeError(baseError: AWSClientRuntime.RestJSONError) throws -> InternalFailureException {
        let reader = baseError.errorBodyReader
        var value = InternalFailureException()
        value.properties.message = try reader["Message"].readIfPresent()
        value.httpResponse = baseError.httpResponse
        value.requestID = baseError.requestID
        value.message = baseError.message
        return value
    }
}

extension ConflictException {

    static func makeError(baseError: AWSClientRuntime.RestJSONError) throws -> ConflictException {
        let reader = baseError.errorBodyReader
        var value = ConflictException()
        value.properties.message = try reader["Message"].readIfPresent()
        value.httpResponse = baseError.httpResponse
        value.requestID = baseError.requestID
        value.message = baseError.message
        return value
    }
}

extension LimitExceededException {

    static func makeError(baseError: AWSClientRuntime.RestJSONError) throws -> LimitExceededException {
        let reader = baseError.errorBodyReader
        var value = LimitExceededException()
        value.properties.message = try reader["Message"].readIfPresent()
        value.httpResponse = baseError.httpResponse
        value.requestID = baseError.requestID
        value.message = baseError.message
        return value
    }
}

extension TranscribeStreamingClientTypes.AudioStream {
    static var marshal: SmithyEventStreamsAPI.MarshalClosure<TranscribeStreamingClientTypes.AudioStream> {
        { (self) in
            var headers: [SmithyEventStreamsAPI.Header] = [.init(name: ":message-type", value: .string("event"))]
            var payload: Foundation.Data? = nil
            switch self {
            case .audioevent(let value):
                headers.append(.init(name: ":event-type", value: .string("AudioEvent")))
                headers.append(.init(name: ":content-type", value: .string("application/octet-stream")))
                payload = value.audioChunk
            case .configurationevent(let value):
                headers.append(.init(name: ":event-type", value: .string("ConfigurationEvent")))
                headers.append(.init(name: ":content-type", value: .string("application/json")))
                let writer = SmithyJSON.Writer(nodeInfo: "")
                try writer["ChannelDefinitions"].write(value.channelDefinitions, with: SmithyReadWrite.listWritingClosure(memberWritingClosure: TranscribeStreamingClientTypes.ChannelDefinition.write(value:to:), memberNodeInfo: "member", isFlattened: false))
                try writer["PostCallAnalyticsSettings"].write(value.postCallAnalyticsSettings, with: TranscribeStreamingClientTypes.PostCallAnalyticsSettings.write(value:to:))
                payload = try writer.data()
            case .sdkUnknown(_):
                throw Smithy.ClientError.unknownError("cannot serialize the unknown event type!")
            }
            return SmithyEventStreamsAPI.Message(headers: headers, payload: payload ?? .init())
        }
    }
}

extension TranscribeStreamingClientTypes.CallAnalyticsTranscriptResultStream {
    static var unmarshal: SmithyEventStreamsAPI.UnmarshalClosure<TranscribeStreamingClientTypes.CallAnalyticsTranscriptResultStream> {
        { message in
            switch try message.type() {
            case .event(let params):
                switch params.eventType {
                case "UtteranceEvent":
                    let value = try SmithyJSON.Reader.readFrom(message.payload, with: TranscribeStreamingClientTypes.UtteranceEvent.read(from:))
                    return .utteranceevent(value)
                case "CategoryEvent":
                    let value = try SmithyJSON.Reader.readFrom(message.payload, with: TranscribeStreamingClientTypes.CategoryEvent.read(from:))
                    return .categoryevent(value)
                default:
                    return .sdkUnknown("error processing event stream, unrecognized event: \(params.eventType)")
                }
            case .exception(let params):
                let makeError: (SmithyEventStreamsAPI.Message, SmithyEventStreamsAPI.MessageType.ExceptionParams) throws -> Swift.Error = { message, params in
                    switch params.exceptionType {
                    case "BadRequestException":
                        let value = try SmithyJSON.Reader.readFrom(message.payload, with: BadRequestException.read(from:))
                        return value
                    case "LimitExceededException":
                        let value = try SmithyJSON.Reader.readFrom(message.payload, with: LimitExceededException.read(from:))
                        return value
                    case "InternalFailureException":
                        let value = try SmithyJSON.Reader.readFrom(message.payload, with: InternalFailureException.read(from:))
                        return value
                    case "ConflictException":
                        let value = try SmithyJSON.Reader.readFrom(message.payload, with: ConflictException.read(from:))
                        return value
                    case "ServiceUnavailableException":
                        let value = try SmithyJSON.Reader.readFrom(message.payload, with: ServiceUnavailableException.read(from:))
                        return value
                    default:
                        let httpResponse = SmithyHTTPAPI.HTTPResponse(body: .data(message.payload), statusCode: .ok)
                        return AWSClientRuntime.UnknownAWSHTTPServiceError(httpResponse: httpResponse, message: "error processing event stream, unrecognized ':exceptionType': \(params.exceptionType); contentType: \(params.contentType ?? "nil")", requestID: nil, typeName: nil)
                    }
                }
                let error = try makeError(message, params)
                throw error
            case .error(let params):
                let httpResponse = SmithyHTTPAPI.HTTPResponse(body: .data(message.payload), statusCode: .ok)
                throw AWSClientRuntime.UnknownAWSHTTPServiceError(httpResponse: httpResponse, message: "error processing event stream, unrecognized ':errorType': \(params.errorCode); message: \(params.message ?? "nil")", requestID: nil, typeName: nil)
            case .unknown(messageType: let messageType):
                throw Smithy.ClientError.unknownError("unrecognized event stream message ':message-type': \(messageType)")
            }
        }
    }
}

extension TranscribeStreamingClientTypes.MedicalTranscriptResultStream {
    static var unmarshal: SmithyEventStreamsAPI.UnmarshalClosure<TranscribeStreamingClientTypes.MedicalTranscriptResultStream> {
        { message in
            switch try message.type() {
            case .event(let params):
                switch params.eventType {
                case "TranscriptEvent":
                    let value = try SmithyJSON.Reader.readFrom(message.payload, with: TranscribeStreamingClientTypes.MedicalTranscriptEvent.read(from:))
                    return .transcriptevent(value)
                default:
                    return .sdkUnknown("error processing event stream, unrecognized event: \(params.eventType)")
                }
            case .exception(let params):
                let makeError: (SmithyEventStreamsAPI.Message, SmithyEventStreamsAPI.MessageType.ExceptionParams) throws -> Swift.Error = { message, params in
                    switch params.exceptionType {
                    case "BadRequestException":
                        let value = try SmithyJSON.Reader.readFrom(message.payload, with: BadRequestException.read(from:))
                        return value
                    case "LimitExceededException":
                        let value = try SmithyJSON.Reader.readFrom(message.payload, with: LimitExceededException.read(from:))
                        return value
                    case "InternalFailureException":
                        let value = try SmithyJSON.Reader.readFrom(message.payload, with: InternalFailureException.read(from:))
                        return value
                    case "ConflictException":
                        let value = try SmithyJSON.Reader.readFrom(message.payload, with: ConflictException.read(from:))
                        return value
                    case "ServiceUnavailableException":
                        let value = try SmithyJSON.Reader.readFrom(message.payload, with: ServiceUnavailableException.read(from:))
                        return value
                    default:
                        let httpResponse = SmithyHTTPAPI.HTTPResponse(body: .data(message.payload), statusCode: .ok)
                        return AWSClientRuntime.UnknownAWSHTTPServiceError(httpResponse: httpResponse, message: "error processing event stream, unrecognized ':exceptionType': \(params.exceptionType); contentType: \(params.contentType ?? "nil")", requestID: nil, typeName: nil)
                    }
                }
                let error = try makeError(message, params)
                throw error
            case .error(let params):
                let httpResponse = SmithyHTTPAPI.HTTPResponse(body: .data(message.payload), statusCode: .ok)
                throw AWSClientRuntime.UnknownAWSHTTPServiceError(httpResponse: httpResponse, message: "error processing event stream, unrecognized ':errorType': \(params.errorCode); message: \(params.message ?? "nil")", requestID: nil, typeName: nil)
            case .unknown(messageType: let messageType):
                throw Smithy.ClientError.unknownError("unrecognized event stream message ':message-type': \(messageType)")
            }
        }
    }
}

extension TranscribeStreamingClientTypes.TranscriptResultStream {
    static var unmarshal: SmithyEventStreamsAPI.UnmarshalClosure<TranscribeStreamingClientTypes.TranscriptResultStream> {
        { message in
            switch try message.type() {
            case .event(let params):
                switch params.eventType {
                case "TranscriptEvent":
                    let value = try SmithyJSON.Reader.readFrom(message.payload, with: TranscribeStreamingClientTypes.TranscriptEvent.read(from:))
                    return .transcriptevent(value)
                default:
                    return .sdkUnknown("error processing event stream, unrecognized event: \(params.eventType)")
                }
            case .exception(let params):
                let makeError: (SmithyEventStreamsAPI.Message, SmithyEventStreamsAPI.MessageType.ExceptionParams) throws -> Swift.Error = { message, params in
                    switch params.exceptionType {
                    case "BadRequestException":
                        let value = try SmithyJSON.Reader.readFrom(message.payload, with: BadRequestException.read(from:))
                        return value
                    case "LimitExceededException":
                        let value = try SmithyJSON.Reader.readFrom(message.payload, with: LimitExceededException.read(from:))
                        return value
                    case "InternalFailureException":
                        let value = try SmithyJSON.Reader.readFrom(message.payload, with: InternalFailureException.read(from:))
                        return value
                    case "ConflictException":
                        let value = try SmithyJSON.Reader.readFrom(message.payload, with: ConflictException.read(from:))
                        return value
                    case "ServiceUnavailableException":
                        let value = try SmithyJSON.Reader.readFrom(message.payload, with: ServiceUnavailableException.read(from:))
                        return value
                    default:
                        let httpResponse = SmithyHTTPAPI.HTTPResponse(body: .data(message.payload), statusCode: .ok)
                        return AWSClientRuntime.UnknownAWSHTTPServiceError(httpResponse: httpResponse, message: "error processing event stream, unrecognized ':exceptionType': \(params.exceptionType); contentType: \(params.contentType ?? "nil")", requestID: nil, typeName: nil)
                    }
                }
                let error = try makeError(message, params)
                throw error
            case .error(let params):
                let httpResponse = SmithyHTTPAPI.HTTPResponse(body: .data(message.payload), statusCode: .ok)
                throw AWSClientRuntime.UnknownAWSHTTPServiceError(httpResponse: httpResponse, message: "error processing event stream, unrecognized ':errorType': \(params.errorCode); message: \(params.message ?? "nil")", requestID: nil, typeName: nil)
            case .unknown(messageType: let messageType):
                throw Smithy.ClientError.unknownError("unrecognized event stream message ':message-type': \(messageType)")
            }
        }
    }
}

extension ServiceUnavailableException {

    static func read(from reader: SmithyJSON.Reader) throws -> ServiceUnavailableException {
        guard reader.hasContent else { throw SmithyReadWrite.ReaderError.requiredValueNotPresent }
        var value = ServiceUnavailableException()
        value.properties.message = try reader["Message"].readIfPresent()
        return value
    }
}

extension ConflictException {

    static func read(from reader: SmithyJSON.Reader) throws -> ConflictException {
        guard reader.hasContent else { throw SmithyReadWrite.ReaderError.requiredValueNotPresent }
        var value = ConflictException()
        value.properties.message = try reader["Message"].readIfPresent()
        return value
    }
}

extension InternalFailureException {

    static func read(from reader: SmithyJSON.Reader) throws -> InternalFailureException {
        guard reader.hasContent else { throw SmithyReadWrite.ReaderError.requiredValueNotPresent }
        var value = InternalFailureException()
        value.properties.message = try reader["Message"].readIfPresent()
        return value
    }
}

extension LimitExceededException {

    static func read(from reader: SmithyJSON.Reader) throws -> LimitExceededException {
        guard reader.hasContent else { throw SmithyReadWrite.ReaderError.requiredValueNotPresent }
        var value = LimitExceededException()
        value.properties.message = try reader["Message"].readIfPresent()
        return value
    }
}

extension BadRequestException {

    static func read(from reader: SmithyJSON.Reader) throws -> BadRequestException {
        guard reader.hasContent else { throw SmithyReadWrite.ReaderError.requiredValueNotPresent }
        var value = BadRequestException()
        value.properties.message = try reader["Message"].readIfPresent()
        return value
    }
}

extension TranscribeStreamingClientTypes.CategoryEvent {

    static func read(from reader: SmithyJSON.Reader) throws -> TranscribeStreamingClientTypes.CategoryEvent {
        guard reader.hasContent else { throw SmithyReadWrite.ReaderError.requiredValueNotPresent }
        var value = TranscribeStreamingClientTypes.CategoryEvent()
        value.matchedCategories = try reader["MatchedCategories"].readListIfPresent(memberReadingClosure: SmithyReadWrite.ReadingClosures.readString(from:), memberNodeInfo: "member", isFlattened: false)
        value.matchedDetails = try reader["MatchedDetails"].readMapIfPresent(valueReadingClosure: TranscribeStreamingClientTypes.PointsOfInterest.read(from:), keyNodeInfo: "key", valueNodeInfo: "value", isFlattened: false)
        return value
    }
}

extension TranscribeStreamingClientTypes.PointsOfInterest {

    static func read(from reader: SmithyJSON.Reader) throws -> TranscribeStreamingClientTypes.PointsOfInterest {
        guard reader.hasContent else { throw SmithyReadWrite.ReaderError.requiredValueNotPresent }
        var value = TranscribeStreamingClientTypes.PointsOfInterest()
        value.timestampRanges = try reader["TimestampRanges"].readListIfPresent(memberReadingClosure: TranscribeStreamingClientTypes.TimestampRange.read(from:), memberNodeInfo: "member", isFlattened: false)
        return value
    }
}

extension TranscribeStreamingClientTypes.TimestampRange {

    static func read(from reader: SmithyJSON.Reader) throws -> TranscribeStreamingClientTypes.TimestampRange {
        guard reader.hasContent else { throw SmithyReadWrite.ReaderError.requiredValueNotPresent }
        var value = TranscribeStreamingClientTypes.TimestampRange()
        value.beginOffsetMillis = try reader["BeginOffsetMillis"].readIfPresent()
        value.endOffsetMillis = try reader["EndOffsetMillis"].readIfPresent()
        return value
    }
}

extension TranscribeStreamingClientTypes.UtteranceEvent {

    static func read(from reader: SmithyJSON.Reader) throws -> TranscribeStreamingClientTypes.UtteranceEvent {
        guard reader.hasContent else { throw SmithyReadWrite.ReaderError.requiredValueNotPresent }
        var value = TranscribeStreamingClientTypes.UtteranceEvent()
        value.utteranceId = try reader["UtteranceId"].readIfPresent()
        value.isPartial = try reader["IsPartial"].readIfPresent() ?? false
        value.participantRole = try reader["ParticipantRole"].readIfPresent()
        value.beginOffsetMillis = try reader["BeginOffsetMillis"].readIfPresent()
        value.endOffsetMillis = try reader["EndOffsetMillis"].readIfPresent()
        value.transcript = try reader["Transcript"].readIfPresent()
        value.items = try reader["Items"].readListIfPresent(memberReadingClosure: TranscribeStreamingClientTypes.CallAnalyticsItem.read(from:), memberNodeInfo: "member", isFlattened: false)
        value.entities = try reader["Entities"].readListIfPresent(memberReadingClosure: TranscribeStreamingClientTypes.CallAnalyticsEntity.read(from:), memberNodeInfo: "member", isFlattened: false)
        value.sentiment = try reader["Sentiment"].readIfPresent()
        value.issuesDetected = try reader["IssuesDetected"].readListIfPresent(memberReadingClosure: TranscribeStreamingClientTypes.IssueDetected.read(from:), memberNodeInfo: "member", isFlattened: false)
        return value
    }
}

extension TranscribeStreamingClientTypes.IssueDetected {

    static func read(from reader: SmithyJSON.Reader) throws -> TranscribeStreamingClientTypes.IssueDetected {
        guard reader.hasContent else { throw SmithyReadWrite.ReaderError.requiredValueNotPresent }
        var value = TranscribeStreamingClientTypes.IssueDetected()
        value.characterOffsets = try reader["CharacterOffsets"].readIfPresent(with: TranscribeStreamingClientTypes.CharacterOffsets.read(from:))
        return value
    }
}

extension TranscribeStreamingClientTypes.CharacterOffsets {

    static func read(from reader: SmithyJSON.Reader) throws -> TranscribeStreamingClientTypes.CharacterOffsets {
        guard reader.hasContent else { throw SmithyReadWrite.ReaderError.requiredValueNotPresent }
        var value = TranscribeStreamingClientTypes.CharacterOffsets()
        value.begin = try reader["Begin"].readIfPresent()
        value.end = try reader["End"].readIfPresent()
        return value
    }
}

extension TranscribeStreamingClientTypes.CallAnalyticsEntity {

    static func read(from reader: SmithyJSON.Reader) throws -> TranscribeStreamingClientTypes.CallAnalyticsEntity {
        guard reader.hasContent else { throw SmithyReadWrite.ReaderError.requiredValueNotPresent }
        var value = TranscribeStreamingClientTypes.CallAnalyticsEntity()
        value.beginOffsetMillis = try reader["BeginOffsetMillis"].readIfPresent()
        value.endOffsetMillis = try reader["EndOffsetMillis"].readIfPresent()
        value.category = try reader["Category"].readIfPresent()
        value.type = try reader["Type"].readIfPresent()
        value.content = try reader["Content"].readIfPresent()
        value.confidence = try reader["Confidence"].readIfPresent()
        return value
    }
}

extension TranscribeStreamingClientTypes.CallAnalyticsItem {

    static func read(from reader: SmithyJSON.Reader) throws -> TranscribeStreamingClientTypes.CallAnalyticsItem {
        guard reader.hasContent else { throw SmithyReadWrite.ReaderError.requiredValueNotPresent }
        var value = TranscribeStreamingClientTypes.CallAnalyticsItem()
        value.beginOffsetMillis = try reader["BeginOffsetMillis"].readIfPresent()
        value.endOffsetMillis = try reader["EndOffsetMillis"].readIfPresent()
        value.type = try reader["Type"].readIfPresent()
        value.content = try reader["Content"].readIfPresent()
        value.confidence = try reader["Confidence"].readIfPresent()
        value.vocabularyFilterMatch = try reader["VocabularyFilterMatch"].readIfPresent() ?? false
        value.stable = try reader["Stable"].readIfPresent()
        return value
    }
}

extension TranscribeStreamingClientTypes.MedicalTranscriptEvent {

    static func read(from reader: SmithyJSON.Reader) throws -> TranscribeStreamingClientTypes.MedicalTranscriptEvent {
        guard reader.hasContent else { throw SmithyReadWrite.ReaderError.requiredValueNotPresent }
        var value = TranscribeStreamingClientTypes.MedicalTranscriptEvent()
        value.transcript = try reader["Transcript"].readIfPresent(with: TranscribeStreamingClientTypes.MedicalTranscript.read(from:))
        return value
    }
}

extension TranscribeStreamingClientTypes.MedicalTranscript {

    static func read(from reader: SmithyJSON.Reader) throws -> TranscribeStreamingClientTypes.MedicalTranscript {
        guard reader.hasContent else { throw SmithyReadWrite.ReaderError.requiredValueNotPresent }
        var value = TranscribeStreamingClientTypes.MedicalTranscript()
        value.results = try reader["Results"].readListIfPresent(memberReadingClosure: TranscribeStreamingClientTypes.MedicalResult.read(from:), memberNodeInfo: "member", isFlattened: false)
        return value
    }
}

extension TranscribeStreamingClientTypes.MedicalResult {

    static func read(from reader: SmithyJSON.Reader) throws -> TranscribeStreamingClientTypes.MedicalResult {
        guard reader.hasContent else { throw SmithyReadWrite.ReaderError.requiredValueNotPresent }
        var value = TranscribeStreamingClientTypes.MedicalResult()
        value.resultId = try reader["ResultId"].readIfPresent()
        value.startTime = try reader["StartTime"].readIfPresent() ?? 0
        value.endTime = try reader["EndTime"].readIfPresent() ?? 0
        value.isPartial = try reader["IsPartial"].readIfPresent() ?? false
        value.alternatives = try reader["Alternatives"].readListIfPresent(memberReadingClosure: TranscribeStreamingClientTypes.MedicalAlternative.read(from:), memberNodeInfo: "member", isFlattened: false)
        value.channelId = try reader["ChannelId"].readIfPresent()
        return value
    }
}

extension TranscribeStreamingClientTypes.MedicalAlternative {

    static func read(from reader: SmithyJSON.Reader) throws -> TranscribeStreamingClientTypes.MedicalAlternative {
        guard reader.hasContent else { throw SmithyReadWrite.ReaderError.requiredValueNotPresent }
        var value = TranscribeStreamingClientTypes.MedicalAlternative()
        value.transcript = try reader["Transcript"].readIfPresent()
        value.items = try reader["Items"].readListIfPresent(memberReadingClosure: TranscribeStreamingClientTypes.MedicalItem.read(from:), memberNodeInfo: "member", isFlattened: false)
        value.entities = try reader["Entities"].readListIfPresent(memberReadingClosure: TranscribeStreamingClientTypes.MedicalEntity.read(from:), memberNodeInfo: "member", isFlattened: false)
        return value
    }
}

extension TranscribeStreamingClientTypes.MedicalEntity {

    static func read(from reader: SmithyJSON.Reader) throws -> TranscribeStreamingClientTypes.MedicalEntity {
        guard reader.hasContent else { throw SmithyReadWrite.ReaderError.requiredValueNotPresent }
        var value = TranscribeStreamingClientTypes.MedicalEntity()
        value.startTime = try reader["StartTime"].readIfPresent() ?? 0
        value.endTime = try reader["EndTime"].readIfPresent() ?? 0
        value.category = try reader["Category"].readIfPresent()
        value.content = try reader["Content"].readIfPresent()
        value.confidence = try reader["Confidence"].readIfPresent()
        return value
    }
}

extension TranscribeStreamingClientTypes.MedicalItem {

    static func read(from reader: SmithyJSON.Reader) throws -> TranscribeStreamingClientTypes.MedicalItem {
        guard reader.hasContent else { throw SmithyReadWrite.ReaderError.requiredValueNotPresent }
        var value = TranscribeStreamingClientTypes.MedicalItem()
        value.startTime = try reader["StartTime"].readIfPresent() ?? 0
        value.endTime = try reader["EndTime"].readIfPresent() ?? 0
        value.type = try reader["Type"].readIfPresent()
        value.content = try reader["Content"].readIfPresent()
        value.confidence = try reader["Confidence"].readIfPresent()
        value.speaker = try reader["Speaker"].readIfPresent()
        return value
    }
}

extension TranscribeStreamingClientTypes.TranscriptEvent {

    static func read(from reader: SmithyJSON.Reader) throws -> TranscribeStreamingClientTypes.TranscriptEvent {
        guard reader.hasContent else { throw SmithyReadWrite.ReaderError.requiredValueNotPresent }
        var value = TranscribeStreamingClientTypes.TranscriptEvent()
        value.transcript = try reader["Transcript"].readIfPresent(with: TranscribeStreamingClientTypes.Transcript.read(from:))
        return value
    }
}

extension TranscribeStreamingClientTypes.Transcript {

    static func read(from reader: SmithyJSON.Reader) throws -> TranscribeStreamingClientTypes.Transcript {
        guard reader.hasContent else { throw SmithyReadWrite.ReaderError.requiredValueNotPresent }
        var value = TranscribeStreamingClientTypes.Transcript()
        value.results = try reader["Results"].readListIfPresent(memberReadingClosure: TranscribeStreamingClientTypes.Result.read(from:), memberNodeInfo: "member", isFlattened: false)
        return value
    }
}

extension TranscribeStreamingClientTypes.Result {

    static func read(from reader: SmithyJSON.Reader) throws -> TranscribeStreamingClientTypes.Result {
        guard reader.hasContent else { throw SmithyReadWrite.ReaderError.requiredValueNotPresent }
        var value = TranscribeStreamingClientTypes.Result()
        value.resultId = try reader["ResultId"].readIfPresent()
        value.startTime = try reader["StartTime"].readIfPresent() ?? 0
        value.endTime = try reader["EndTime"].readIfPresent() ?? 0
        value.isPartial = try reader["IsPartial"].readIfPresent() ?? false
        value.alternatives = try reader["Alternatives"].readListIfPresent(memberReadingClosure: TranscribeStreamingClientTypes.Alternative.read(from:), memberNodeInfo: "member", isFlattened: false)
        value.channelId = try reader["ChannelId"].readIfPresent()
        value.languageCode = try reader["LanguageCode"].readIfPresent()
        value.languageIdentification = try reader["LanguageIdentification"].readListIfPresent(memberReadingClosure: TranscribeStreamingClientTypes.LanguageWithScore.read(from:), memberNodeInfo: "member", isFlattened: false)
        return value
    }
}

extension TranscribeStreamingClientTypes.LanguageWithScore {

    static func read(from reader: SmithyJSON.Reader) throws -> TranscribeStreamingClientTypes.LanguageWithScore {
        guard reader.hasContent else { throw SmithyReadWrite.ReaderError.requiredValueNotPresent }
        var value = TranscribeStreamingClientTypes.LanguageWithScore()
        value.languageCode = try reader["LanguageCode"].readIfPresent()
        value.score = try reader["Score"].readIfPresent() ?? 0
        return value
    }
}

extension TranscribeStreamingClientTypes.Alternative {

    static func read(from reader: SmithyJSON.Reader) throws -> TranscribeStreamingClientTypes.Alternative {
        guard reader.hasContent else { throw SmithyReadWrite.ReaderError.requiredValueNotPresent }
        var value = TranscribeStreamingClientTypes.Alternative()
        value.transcript = try reader["Transcript"].readIfPresent()
        value.items = try reader["Items"].readListIfPresent(memberReadingClosure: TranscribeStreamingClientTypes.Item.read(from:), memberNodeInfo: "member", isFlattened: false)
        value.entities = try reader["Entities"].readListIfPresent(memberReadingClosure: TranscribeStreamingClientTypes.Entity.read(from:), memberNodeInfo: "member", isFlattened: false)
        return value
    }
}

extension TranscribeStreamingClientTypes.Entity {

    static func read(from reader: SmithyJSON.Reader) throws -> TranscribeStreamingClientTypes.Entity {
        guard reader.hasContent else { throw SmithyReadWrite.ReaderError.requiredValueNotPresent }
        var value = TranscribeStreamingClientTypes.Entity()
        value.startTime = try reader["StartTime"].readIfPresent() ?? 0
        value.endTime = try reader["EndTime"].readIfPresent() ?? 0
        value.category = try reader["Category"].readIfPresent()
        value.type = try reader["Type"].readIfPresent()
        value.content = try reader["Content"].readIfPresent()
        value.confidence = try reader["Confidence"].readIfPresent()
        return value
    }
}

extension TranscribeStreamingClientTypes.Item {

    static func read(from reader: SmithyJSON.Reader) throws -> TranscribeStreamingClientTypes.Item {
        guard reader.hasContent else { throw SmithyReadWrite.ReaderError.requiredValueNotPresent }
        var value = TranscribeStreamingClientTypes.Item()
        value.startTime = try reader["StartTime"].readIfPresent() ?? 0
        value.endTime = try reader["EndTime"].readIfPresent() ?? 0
        value.type = try reader["Type"].readIfPresent()
        value.content = try reader["Content"].readIfPresent()
        value.vocabularyFilterMatch = try reader["VocabularyFilterMatch"].readIfPresent() ?? false
        value.speaker = try reader["Speaker"].readIfPresent()
        value.confidence = try reader["Confidence"].readIfPresent()
        value.stable = try reader["Stable"].readIfPresent()
        return value
    }
}

extension TranscribeStreamingClientTypes.ConfigurationEvent {

    static func write(value: TranscribeStreamingClientTypes.ConfigurationEvent?, to writer: SmithyJSON.Writer) throws {
        guard let value else { return }
        try writer["ChannelDefinitions"].writeList(value.channelDefinitions, memberWritingClosure: TranscribeStreamingClientTypes.ChannelDefinition.write(value:to:), memberNodeInfo: "member", isFlattened: false)
        try writer["PostCallAnalyticsSettings"].write(value.postCallAnalyticsSettings, with: TranscribeStreamingClientTypes.PostCallAnalyticsSettings.write(value:to:))
    }
}

extension TranscribeStreamingClientTypes.PostCallAnalyticsSettings {

    static func write(value: TranscribeStreamingClientTypes.PostCallAnalyticsSettings?, to writer: SmithyJSON.Writer) throws {
        guard let value else { return }
        try writer["ContentRedactionOutput"].write(value.contentRedactionOutput)
        try writer["DataAccessRoleArn"].write(value.dataAccessRoleArn)
        try writer["OutputEncryptionKMSKeyId"].write(value.outputEncryptionKMSKeyId)
        try writer["OutputLocation"].write(value.outputLocation)
    }
}

extension TranscribeStreamingClientTypes.ChannelDefinition {

    static func write(value: TranscribeStreamingClientTypes.ChannelDefinition?, to writer: SmithyJSON.Writer) throws {
        guard let value else { return }
        try writer["ChannelId"].write(value.channelId)
        try writer["ParticipantRole"].write(value.participantRole)
    }
}

extension TranscribeStreamingClientTypes.AudioEvent {

    static func write(value: TranscribeStreamingClientTypes.AudioEvent?, to writer: SmithyJSON.Writer) throws {
        guard let value else { return }
        try writer["AudioChunk"].write(value.audioChunk)
    }
}

public enum TranscribeStreamingClientTypes {}
